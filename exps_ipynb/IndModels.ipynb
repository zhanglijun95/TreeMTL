{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from main.head import ASPPHeadNode\n",
    "from data.nyuv2_dataloader_adashare import NYU_v2\n",
    "from data.pixel2pixel_loss import NYUCriterions\n",
    "from data.pixel2pixel_metrics import NYUMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeplab Resnet - Independent Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine_par = True\n",
    "\n",
    "def conv3x3(in_channels, out_channels, stride=1, dilation=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "\n",
    "    kernel_size = np.asarray((3, 3))\n",
    "\n",
    "    # Compute the size of the upsampled filter with\n",
    "    # a specified dilation rate.\n",
    "    upsampled_kernel_size = (kernel_size - 1) * (dilation - 1) + kernel_size\n",
    "\n",
    "    # Determine the padding that is necessary for full padding,\n",
    "    # meaning the output spatial size is equal to input spatial size\n",
    "    full_padding = (upsampled_kernel_size - 1) // 2\n",
    "\n",
    "    # Conv2d doesn't accept numpy arrays as arguments\n",
    "    full_padding, kernel_size = tuple(full_padding), tuple(kernel_size)\n",
    "\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=full_padding, dilation=dilation, bias=False)\n",
    "\n",
    "# No projection: identity shortcut\n",
    "# conv -> bn -> relu -> conv -> bn\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, dilation=1):\n",
    "        super(BasicBlock, self).__init__() \n",
    "        self.conv1 = conv3x3(inplanes, planes, stride, dilation=dilation)\n",
    "        self.bn1 = nn.BatchNorm2d(planes, affine = affine_par)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, dilation=dilation)\n",
    "        self.bn2 = nn.BatchNorm2d(planes, affine = affine_par)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        y = self.bn2(out)\n",
    "        return y\n",
    "    \n",
    "# Add residual projection\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, block, ds):\n",
    "        super(ResidualBlock, self).__init__() \n",
    "        self.block = block\n",
    "        self.ds = ds\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = self.ds(x) if self.ds is not None else x\n",
    "        x = F.relu(residual + self.block(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deeplab_ResNet_Backbone(nn.Module):\n",
    "    def __init__(self, block, layers):\n",
    "        super(Deeplab_ResNet_Backbone, self).__init__()\n",
    "        \n",
    "        self.inplanes = 64\n",
    "\n",
    "        strides = [1, 2, 1, 1]\n",
    "        dilations = [1, 1, 2, 4]\n",
    "        filt_sizes = [64, 128, 256, 512]\n",
    "        self.blocks = []\n",
    "        self.layer_config = layers\n",
    "        \n",
    "        branch_cnt = 0\n",
    "        seed = self._make_seed()\n",
    "        self.__add_to_blocks(seed)\n",
    "        branch_cnt += 1\n",
    "        \n",
    "        for segment, num_blocks in enumerate(self.layer_config):\n",
    "            filt_size, num_blocks, stride, dilation = filt_sizes[segment],layers[segment],strides[segment],dilations[segment]\n",
    "            for b_idx in range(num_blocks):\n",
    "                blocklayer = self._make_blocklayer(b_idx, block, filt_size, stride=stride, dilation=dilation)\n",
    "                self.__add_to_blocks(blocklayer)\n",
    "\n",
    "        self.blocks = nn.ModuleList(self.blocks)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_seed(self):\n",
    "        seed = nn.Sequential(nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "                             nn.BatchNorm2d(64, affine=affine_par),\n",
    "                             nn.ReLU(inplace=True),\n",
    "                             nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True)) \n",
    "        return seed\n",
    "    \n",
    "    def _make_downsample(self, block, inplanes, planes, stride=1, dilation=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or inplanes != planes * block.expansion or dilation == 2 or dilation == 4:\n",
    "            downsample = nn.Sequential(nn.Conv2d(inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                                       nn.BatchNorm2d(planes * block.expansion, affine = affine_par))\n",
    "        return downsample\n",
    "    \n",
    "    def _make_blocklayer(self, block_idx, block, planes, stride=1, dilation=1):\n",
    "        ds = None\n",
    "        if block_idx == 0:\n",
    "            basic_block = block(self.inplanes, planes, stride, dilation=dilation)\n",
    "            ds = self._make_downsample(block, self.inplanes, planes, stride=stride, dilation=dilation)\n",
    "            self.inplanes = planes * block.expansion\n",
    "        else:\n",
    "            basic_block = block(self.inplanes, planes, dilation=dilation)\n",
    "            \n",
    "        blocklayer = ResidualBlock(basic_block, ds)\n",
    "        return blocklayer\n",
    "    \n",
    "    def __add_to_blocks(self, block):\n",
    "        self.blocks.append(block)\n",
    "        return\n",
    "\n",
    "    def forward(self, x): \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deeplab_ASPP(nn.Module):\n",
    "    def __init__(self, cls_num):\n",
    "        super(Deeplab_ASPP, self).__init__()\n",
    "        self.branch = 100\n",
    "        self.backbone = Deeplab_ResNet_Backbone(BasicBlock, [3, 4, 6, 3])\n",
    "        self.heads = ASPPHeadNode(512, cls_num)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feature = self.backbone(x)\n",
    "        output = self.heads(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on NYUv2 for one task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, task, train_dataloader, val_dataloader, criterion, metric, \n",
    "                 lr=0.001, decay_lr_freq=4000, decay_lr_rate=0.5,\n",
    "                 print_iters=50, val_iters=200, save_iters=200):\n",
    "        super(Trainer, self).__init__()\n",
    "        self.model = model\n",
    "        self.task = task\n",
    "        self.startIter = 0\n",
    "        self.optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=lr, betas=(0.5, 0.999), weight_decay=0.0001)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=decay_lr_freq, gamma=decay_lr_rate)\n",
    "        \n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.train_iter = iter(self.train_dataloader)\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.criterion = criterion\n",
    "        self.metric = metric\n",
    "        \n",
    "        self.loss_list = {}\n",
    "        self.set_train_loss()\n",
    "        \n",
    "        self.print_iters = print_iters\n",
    "        self.val_iters = val_iters\n",
    "        self.save_iters = save_iters\n",
    "    \n",
    "    def train(self, iters, savePath=None, reload=None):\n",
    "        self.model.train()\n",
    "        if reload is not None and savePath is not None:\n",
    "            self.load_model(savePath, reload)\n",
    "\n",
    "        for i in range(self.startIter, iters):\n",
    "            self.train_step()\n",
    "\n",
    "            if (i+1) % self.print_iters == 0:\n",
    "                self.print_train_loss(i)\n",
    "                self.set_train_loss()\n",
    "            if (i+1) % self.val_iters == 0:\n",
    "                self.validate(i)\n",
    "            if (i+1) % self.save_iters == 0:\n",
    "                if savePath is not None:\n",
    "                    self.save_model(i, savePath)\n",
    "            \n",
    "        # Reset loss list and the data iters\n",
    "        self.set_train_loss()\n",
    "        return\n",
    "    \n",
    "    def train_step(self):\n",
    "        self.model.train()\n",
    "        try:\n",
    "            data = next(self.train_iter)\n",
    "        except StopIteration:\n",
    "            self.train_iter = iter(self.train_dataloader)\n",
    "            data = next(self.train_iter)\n",
    "            \n",
    "        x = data['input'].cuda()\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.model(x)\n",
    "        \n",
    "        loss = 0\n",
    "        y = data[self.task].cuda()\n",
    "        if self.task + '_mask' in data:\n",
    "            loss = self.criterion(output, y, data[self.task + '_mask'].cuda())\n",
    "        else:\n",
    "            loss = self.criterion(output, y)\n",
    "\n",
    "        self.loss_list.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step()\n",
    "        return\n",
    "    \n",
    "    def validate(self, it):\n",
    "        self.model.eval()\n",
    "        loss_list = {}\n",
    "        loss_list = []\n",
    "        \n",
    "        for i, data in enumerate(self.val_dataloader):\n",
    "            x = data['input'].cuda()\n",
    "            output = self.model(x)\n",
    "\n",
    "            y = data[self.task].cuda()\n",
    "            if self.task + '_mask' in data:\n",
    "                tloss = self.criterion(output, y, data[self.task + '_mask'].cuda())\n",
    "                self.metric(output, y, data[self.task + '_mask'].cuda())\n",
    "            else:\n",
    "                tloss = self.criterion(output, y)\n",
    "                self.metric(output, y)\n",
    "            loss_list.append(tloss.item())\n",
    "\n",
    "        val_results = self.metric.val_metrics()\n",
    "        print('[Iter {} Task {}] Val Loss: {:.4f}'.format((it+1), self.task[:4], np.mean(loss_list)), flush=True)\n",
    "        print(val_results, flush=True)\n",
    "        print('======================================================================', flush=True)\n",
    "        return\n",
    "    \n",
    "    # helper functions\n",
    "    def set_train_loss(self):\n",
    "        self.loss_list = []\n",
    "        return\n",
    "    \n",
    "    def load_model(self, savePath, reload):\n",
    "        state = torch.load(savePath + reload)\n",
    "        self.startIter = state['iter'] + 1\n",
    "        self.model.load_state_dict(state['state_dict'])\n",
    "        self.optimizer.load_state_dict(state['optimizer'])\n",
    "        self.scheduler.load_state_dict(state['scheduler'])\n",
    "        return\n",
    "    \n",
    "    def save_model(self, it, savePath):\n",
    "        state = {'iter': it,\n",
    "                'state_dict': self.model.state_dict(),\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "                'scheduler': self.scheduler.state_dict()}\n",
    "        torch.save(state, savePath + self.task + '.model')\n",
    "        return\n",
    "    \n",
    "    def print_train_loss(self, it):\n",
    "        if self.loss_list:\n",
    "            avg_loss = np.mean(self.loss_list)\n",
    "        else:\n",
    "            return\n",
    "        print('[Iter {} Task {}] Train Loss: {:.4f}'.format((it+1), self.task[:4], avg_loss), flush=True)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = '/mnt/nfs/work1/huiguan/lijunzhang/policymtl/data/NYUv2/'\n",
    "tasks = ('segment_semantic','normal','depth_zbuffer')\n",
    "task_cls_num = {'segment_semantic': 40, 'normal':3, 'depth_zbuffer': 1}\n",
    "\n",
    "\n",
    "criterionDict = {}\n",
    "metricDict = {}\n",
    "clsNum = {}\n",
    "task = ['segment_semantic']\n",
    "dataset = NYU_v2(dataroot, 'train', crop_h=321, crop_w=321)\n",
    "trainDataloader = DataLoader(dataset, 16, shuffle=True)\n",
    "\n",
    "dataset = NYU_v2(dataroot, 'test', crop_h=321, crop_w=321)\n",
    "valDataloader = DataLoader(dataset, 16, shuffle=True)\n",
    "criterion = NYUCriterions(task[0])\n",
    "metric = NYUMetrics(task[0])\n",
    "clsNum = task_cls_num[task[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Deeplab_ASPP(clsNum)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 50 Task segm] Train Loss: 3.4750\n",
      "[Iter 100 Task segm] Train Loss: 2.8563\n",
      "[Iter 150 Task segm] Train Loss: 2.7555\n",
      "[Iter 200 Task segm] Train Loss: 2.7052\n",
      "[Iter 200 Task segm] Val Loss: 2.7961\n",
      "{'mIoU': 0.1649, 'Pixel Acc': 0.3115}\n",
      "======================================================================\n",
      "[Iter 250 Task segm] Train Loss: 2.6486\n",
      "[Iter 300 Task segm] Train Loss: 2.6110\n",
      "[Iter 350 Task segm] Train Loss: 2.6516\n",
      "[Iter 400 Task segm] Train Loss: 2.6310\n",
      "[Iter 400 Task segm] Val Loss: 2.6000\n",
      "{'mIoU': 0.1114, 'Pixel Acc': 0.3518}\n",
      "======================================================================\n",
      "[Iter 450 Task segm] Train Loss: 2.5912\n",
      "[Iter 500 Task segm] Train Loss: 2.5425\n",
      "[Iter 550 Task segm] Train Loss: 2.5495\n",
      "[Iter 600 Task segm] Train Loss: 2.5162\n",
      "[Iter 600 Task segm] Val Loss: 2.6940\n",
      "{'mIoU': 0.1576, 'Pixel Acc': 0.3201}\n",
      "======================================================================\n",
      "[Iter 650 Task segm] Train Loss: 2.5236\n",
      "[Iter 700 Task segm] Train Loss: 2.4928\n",
      "[Iter 750 Task segm] Train Loss: 2.5000\n",
      "[Iter 800 Task segm] Train Loss: 2.4932\n",
      "[Iter 800 Task segm] Val Loss: 2.4027\n",
      "{'mIoU': 0.1577, 'Pixel Acc': 0.3636}\n",
      "======================================================================\n",
      "[Iter 850 Task segm] Train Loss: 2.4692\n",
      "[Iter 900 Task segm] Train Loss: 2.4413\n",
      "[Iter 950 Task segm] Train Loss: 2.4546\n",
      "[Iter 1000 Task segm] Train Loss: 2.3824\n",
      "[Iter 1000 Task segm] Val Loss: 2.4556\n",
      "{'mIoU': 0.1047, 'Pixel Acc': 0.3633}\n",
      "======================================================================\n",
      "[Iter 1050 Task segm] Train Loss: 2.4080\n",
      "[Iter 1100 Task segm] Train Loss: 2.4135\n",
      "[Iter 1150 Task segm] Train Loss: 2.3906\n",
      "[Iter 1200 Task segm] Train Loss: 2.3679\n",
      "[Iter 1200 Task segm] Val Loss: 2.4873\n",
      "{'mIoU': 0.1035, 'Pixel Acc': 0.3603}\n",
      "======================================================================\n",
      "[Iter 1250 Task segm] Train Loss: 2.3743\n",
      "[Iter 1300 Task segm] Train Loss: 2.3721\n",
      "[Iter 1350 Task segm] Train Loss: 2.2912\n",
      "[Iter 1400 Task segm] Train Loss: 2.3388\n",
      "[Iter 1400 Task segm] Val Loss: 2.3667\n",
      "{'mIoU': 0.0852, 'Pixel Acc': 0.3596}\n",
      "======================================================================\n",
      "[Iter 1450 Task segm] Train Loss: 2.2787\n",
      "[Iter 1500 Task segm] Train Loss: 2.2881\n",
      "[Iter 1550 Task segm] Train Loss: 2.2826\n",
      "[Iter 1600 Task segm] Train Loss: 2.2326\n",
      "[Iter 1600 Task segm] Val Loss: 2.4009\n",
      "{'mIoU': 0.0798, 'Pixel Acc': 0.3651}\n",
      "======================================================================\n",
      "[Iter 1650 Task segm] Train Loss: 2.2559\n",
      "[Iter 1700 Task segm] Train Loss: 2.2711\n",
      "[Iter 1750 Task segm] Train Loss: 2.2131\n",
      "[Iter 1800 Task segm] Train Loss: 2.1741\n",
      "[Iter 1800 Task segm] Val Loss: 2.2170\n",
      "{'mIoU': 0.1114, 'Pixel Acc': 0.4016}\n",
      "======================================================================\n",
      "[Iter 1850 Task segm] Train Loss: 2.1761\n",
      "[Iter 1900 Task segm] Train Loss: 2.1689\n",
      "[Iter 1950 Task segm] Train Loss: 2.1762\n",
      "[Iter 2000 Task segm] Train Loss: 2.1025\n",
      "[Iter 2000 Task segm] Val Loss: 2.1893\n",
      "{'mIoU': 0.0938, 'Pixel Acc': 0.3992}\n",
      "======================================================================\n",
      "[Iter 2050 Task segm] Train Loss: 2.0709\n",
      "[Iter 2100 Task segm] Train Loss: 2.1225\n",
      "[Iter 2150 Task segm] Train Loss: 2.1046\n",
      "[Iter 2200 Task segm] Train Loss: 2.1506\n",
      "[Iter 2200 Task segm] Val Loss: 2.1719\n",
      "{'mIoU': 0.0982, 'Pixel Acc': 0.4198}\n",
      "======================================================================\n",
      "[Iter 2250 Task segm] Train Loss: 2.0735\n",
      "[Iter 2300 Task segm] Train Loss: 2.0331\n",
      "[Iter 2350 Task segm] Train Loss: 2.0087\n",
      "[Iter 2400 Task segm] Train Loss: 2.0375\n",
      "[Iter 2400 Task segm] Val Loss: 2.1944\n",
      "{'mIoU': 0.0855, 'Pixel Acc': 0.4023}\n",
      "======================================================================\n",
      "[Iter 2450 Task segm] Train Loss: 2.0038\n",
      "[Iter 2500 Task segm] Train Loss: 1.9682\n",
      "[Iter 2550 Task segm] Train Loss: 1.9536\n",
      "[Iter 2600 Task segm] Train Loss: 1.9772\n",
      "[Iter 2600 Task segm] Val Loss: 2.1358\n",
      "{'mIoU': 0.1075, 'Pixel Acc': 0.4227}\n",
      "======================================================================\n",
      "[Iter 2650 Task segm] Train Loss: 1.9157\n",
      "[Iter 2700 Task segm] Train Loss: 1.9226\n",
      "[Iter 2750 Task segm] Train Loss: 1.8442\n",
      "[Iter 2800 Task segm] Train Loss: 1.8875\n",
      "[Iter 2800 Task segm] Val Loss: 2.3531\n",
      "{'mIoU': 0.0963, 'Pixel Acc': 0.3498}\n",
      "======================================================================\n",
      "[Iter 2850 Task segm] Train Loss: 1.9057\n",
      "[Iter 2900 Task segm] Train Loss: 1.8920\n",
      "[Iter 2950 Task segm] Train Loss: 1.8927\n",
      "[Iter 3000 Task segm] Train Loss: 1.8563\n",
      "[Iter 3000 Task segm] Val Loss: 1.9706\n",
      "{'mIoU': 0.1349, 'Pixel Acc': 0.4584}\n",
      "======================================================================\n",
      "[Iter 3050 Task segm] Train Loss: 1.8060\n",
      "[Iter 3100 Task segm] Train Loss: 1.8377\n",
      "[Iter 3150 Task segm] Train Loss: 1.8025\n",
      "[Iter 3200 Task segm] Train Loss: 1.7951\n",
      "[Iter 3200 Task segm] Val Loss: 1.9491\n",
      "{'mIoU': 0.1211, 'Pixel Acc': 0.4496}\n",
      "======================================================================\n",
      "[Iter 3250 Task segm] Train Loss: 1.7770\n",
      "[Iter 3300 Task segm] Train Loss: 1.7606\n",
      "[Iter 3350 Task segm] Train Loss: 1.7799\n",
      "[Iter 3400 Task segm] Train Loss: 1.7406\n",
      "[Iter 3400 Task segm] Val Loss: 1.9729\n",
      "{'mIoU': 0.1361, 'Pixel Acc': 0.4584}\n",
      "======================================================================\n",
      "[Iter 3450 Task segm] Train Loss: 1.7095\n",
      "[Iter 3500 Task segm] Train Loss: 1.7191\n",
      "[Iter 3550 Task segm] Train Loss: 1.7052\n",
      "[Iter 3600 Task segm] Train Loss: 1.7619\n",
      "[Iter 3600 Task segm] Val Loss: 1.8422\n",
      "{'mIoU': 0.1572, 'Pixel Acc': 0.4905}\n",
      "======================================================================\n",
      "[Iter 3650 Task segm] Train Loss: 1.7118\n",
      "[Iter 3700 Task segm] Train Loss: 1.7168\n",
      "[Iter 3750 Task segm] Train Loss: 1.6791\n",
      "[Iter 3800 Task segm] Train Loss: 1.6737\n",
      "[Iter 3800 Task segm] Val Loss: 3.7117\n",
      "{'mIoU': 0.1045, 'Pixel Acc': 0.3719}\n",
      "======================================================================\n",
      "[Iter 3850 Task segm] Train Loss: 1.6565\n",
      "[Iter 3900 Task segm] Train Loss: 1.6560\n",
      "[Iter 3950 Task segm] Train Loss: 1.7190\n",
      "[Iter 4000 Task segm] Train Loss: 1.6444\n",
      "[Iter 4000 Task segm] Val Loss: 1.8059\n",
      "{'mIoU': 0.1618, 'Pixel Acc': 0.4939}\n",
      "======================================================================\n",
      "[Iter 4050 Task segm] Train Loss: 1.5289\n",
      "[Iter 4100 Task segm] Train Loss: 1.5262\n",
      "[Iter 4150 Task segm] Train Loss: 1.4822\n",
      "[Iter 4200 Task segm] Train Loss: 1.4882\n",
      "[Iter 4200 Task segm] Val Loss: 1.7436\n",
      "{'mIoU': 0.1755, 'Pixel Acc': 0.5006}\n",
      "======================================================================\n",
      "[Iter 4250 Task segm] Train Loss: 1.4868\n",
      "[Iter 4300 Task segm] Train Loss: 1.4907\n",
      "[Iter 4350 Task segm] Train Loss: 1.4571\n",
      "[Iter 4400 Task segm] Train Loss: 1.4408\n",
      "[Iter 4400 Task segm] Val Loss: 1.7855\n",
      "{'mIoU': 0.1779, 'Pixel Acc': 0.4992}\n",
      "======================================================================\n",
      "[Iter 4450 Task segm] Train Loss: 1.4466\n",
      "[Iter 4500 Task segm] Train Loss: 1.4103\n",
      "[Iter 4550 Task segm] Train Loss: 1.4110\n",
      "[Iter 4600 Task segm] Train Loss: 1.3940\n",
      "[Iter 4600 Task segm] Val Loss: 1.7617\n",
      "{'mIoU': 0.1784, 'Pixel Acc': 0.497}\n",
      "======================================================================\n",
      "[Iter 4650 Task segm] Train Loss: 1.3805\n",
      "[Iter 4700 Task segm] Train Loss: 1.3618\n",
      "[Iter 4750 Task segm] Train Loss: 1.3658\n",
      "[Iter 4800 Task segm] Train Loss: 1.3743\n",
      "[Iter 4800 Task segm] Val Loss: 1.6780\n",
      "{'mIoU': 0.1945, 'Pixel Acc': 0.5203}\n",
      "======================================================================\n",
      "[Iter 4850 Task segm] Train Loss: 1.3614\n",
      "[Iter 4900 Task segm] Train Loss: 1.3499\n",
      "[Iter 4950 Task segm] Train Loss: 1.3056\n",
      "[Iter 5000 Task segm] Train Loss: 1.3428\n",
      "[Iter 5000 Task segm] Val Loss: 1.6570\n",
      "{'mIoU': 0.1961, 'Pixel Acc': 0.5242}\n",
      "======================================================================\n",
      "[Iter 5050 Task segm] Train Loss: 1.3114\n",
      "[Iter 5100 Task segm] Train Loss: 1.3068\n",
      "[Iter 5150 Task segm] Train Loss: 1.2858\n",
      "[Iter 5200 Task segm] Train Loss: 1.2761\n",
      "[Iter 5200 Task segm] Val Loss: 1.6960\n",
      "{'mIoU': 0.1983, 'Pixel Acc': 0.5129}\n",
      "======================================================================\n",
      "[Iter 5250 Task segm] Train Loss: 1.3018\n",
      "[Iter 5300 Task segm] Train Loss: 1.3040\n",
      "[Iter 5350 Task segm] Train Loss: 1.2822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 5400 Task segm] Train Loss: 1.2827\n",
      "[Iter 5400 Task segm] Val Loss: 1.7155\n",
      "{'mIoU': 0.1765, 'Pixel Acc': 0.5047}\n",
      "======================================================================\n",
      "[Iter 5450 Task segm] Train Loss: 1.2524\n",
      "[Iter 5500 Task segm] Train Loss: 1.2277\n",
      "[Iter 5550 Task segm] Train Loss: 1.2514\n",
      "[Iter 5600 Task segm] Train Loss: 1.2607\n",
      "[Iter 5600 Task segm] Val Loss: 1.6373\n",
      "{'mIoU': 0.21, 'Pixel Acc': 0.5268}\n",
      "======================================================================\n",
      "[Iter 5650 Task segm] Train Loss: 1.2171\n",
      "[Iter 5700 Task segm] Train Loss: 1.2621\n",
      "[Iter 5750 Task segm] Train Loss: 1.2171\n",
      "[Iter 5800 Task segm] Train Loss: 1.2169\n",
      "[Iter 5800 Task segm] Val Loss: 1.6236\n",
      "{'mIoU': 0.2091, 'Pixel Acc': 0.5292}\n",
      "======================================================================\n",
      "[Iter 5850 Task segm] Train Loss: 1.2013\n",
      "[Iter 5900 Task segm] Train Loss: 1.2036\n",
      "[Iter 5950 Task segm] Train Loss: 1.2018\n",
      "[Iter 6000 Task segm] Train Loss: 1.1582\n",
      "[Iter 6000 Task segm] Val Loss: 1.6139\n",
      "{'mIoU': 0.2037, 'Pixel Acc': 0.528}\n",
      "======================================================================\n",
      "[Iter 6050 Task segm] Train Loss: 1.1488\n",
      "[Iter 6100 Task segm] Train Loss: 1.1792\n",
      "[Iter 6150 Task segm] Train Loss: 1.1426\n",
      "[Iter 6200 Task segm] Train Loss: 1.1362\n",
      "[Iter 6200 Task segm] Val Loss: 1.7063\n",
      "{'mIoU': 0.1937, 'Pixel Acc': 0.511}\n",
      "======================================================================\n",
      "[Iter 6250 Task segm] Train Loss: 1.1499\n",
      "[Iter 6300 Task segm] Train Loss: 1.1111\n",
      "[Iter 6350 Task segm] Train Loss: 1.0956\n",
      "[Iter 6400 Task segm] Train Loss: 1.1394\n",
      "[Iter 6400 Task segm] Val Loss: 1.7100\n",
      "{'mIoU': 0.1873, 'Pixel Acc': 0.5063}\n",
      "======================================================================\n",
      "[Iter 6450 Task segm] Train Loss: 1.0921\n",
      "[Iter 6500 Task segm] Train Loss: 1.1130\n",
      "[Iter 6550 Task segm] Train Loss: 1.1003\n",
      "[Iter 6600 Task segm] Train Loss: 1.1126\n",
      "[Iter 6600 Task segm] Val Loss: 1.8434\n",
      "{'mIoU': 0.1942, 'Pixel Acc': 0.5047}\n",
      "======================================================================\n",
      "[Iter 6650 Task segm] Train Loss: 1.0842\n",
      "[Iter 6700 Task segm] Train Loss: 1.0688\n",
      "[Iter 6750 Task segm] Train Loss: 1.0848\n",
      "[Iter 6800 Task segm] Train Loss: 1.0870\n",
      "[Iter 6800 Task segm] Val Loss: 1.7393\n",
      "{'mIoU': 0.1824, 'Pixel Acc': 0.5012}\n",
      "======================================================================\n",
      "[Iter 6850 Task segm] Train Loss: 1.0586\n",
      "[Iter 6900 Task segm] Train Loss: 1.0402\n",
      "[Iter 6950 Task segm] Train Loss: 1.0538\n",
      "[Iter 7000 Task segm] Train Loss: 1.0254\n",
      "[Iter 7000 Task segm] Val Loss: 1.6838\n",
      "{'mIoU': 0.202, 'Pixel Acc': 0.5204}\n",
      "======================================================================\n",
      "[Iter 7050 Task segm] Train Loss: 1.0667\n",
      "[Iter 7100 Task segm] Train Loss: 1.0724\n",
      "[Iter 7150 Task segm] Train Loss: 1.0991\n",
      "[Iter 7200 Task segm] Train Loss: 1.0466\n",
      "[Iter 7200 Task segm] Val Loss: 1.6423\n",
      "{'mIoU': 0.2092, 'Pixel Acc': 0.5218}\n",
      "======================================================================\n",
      "[Iter 7250 Task segm] Train Loss: 1.0207\n",
      "[Iter 7300 Task segm] Train Loss: 1.0365\n",
      "[Iter 7350 Task segm] Train Loss: 0.9922\n",
      "[Iter 7400 Task segm] Train Loss: 0.9914\n",
      "[Iter 7400 Task segm] Val Loss: 1.5956\n",
      "{'mIoU': 0.2043, 'Pixel Acc': 0.537}\n",
      "======================================================================\n",
      "[Iter 7450 Task segm] Train Loss: 1.0271\n",
      "[Iter 7500 Task segm] Train Loss: 0.9738\n",
      "[Iter 7550 Task segm] Train Loss: 1.0207\n",
      "[Iter 7600 Task segm] Train Loss: 0.9683\n",
      "[Iter 7600 Task segm] Val Loss: 1.6374\n",
      "{'mIoU': 0.219, 'Pixel Acc': 0.5319}\n",
      "======================================================================\n",
      "[Iter 7650 Task segm] Train Loss: 0.9687\n",
      "[Iter 7700 Task segm] Train Loss: 0.9534\n",
      "[Iter 7750 Task segm] Train Loss: 0.9434\n",
      "[Iter 7800 Task segm] Train Loss: 0.9453\n",
      "[Iter 7800 Task segm] Val Loss: 1.5793\n",
      "{'mIoU': 0.2124, 'Pixel Acc': 0.535}\n",
      "======================================================================\n",
      "[Iter 7850 Task segm] Train Loss: 0.9433\n",
      "[Iter 7900 Task segm] Train Loss: 0.9277\n",
      "[Iter 7950 Task segm] Train Loss: 0.9369\n",
      "[Iter 8000 Task segm] Train Loss: 0.9200\n",
      "[Iter 8000 Task segm] Val Loss: 1.6043\n",
      "{'mIoU': 0.2222, 'Pixel Acc': 0.5322}\n",
      "======================================================================\n",
      "[Iter 8050 Task segm] Train Loss: 0.8728\n",
      "[Iter 8100 Task segm] Train Loss: 0.8614\n",
      "[Iter 8150 Task segm] Train Loss: 0.8462\n",
      "[Iter 8200 Task segm] Train Loss: 0.8299\n",
      "[Iter 8200 Task segm] Val Loss: 1.5183\n",
      "{'mIoU': 0.2333, 'Pixel Acc': 0.5576}\n",
      "======================================================================\n",
      "[Iter 8250 Task segm] Train Loss: 0.7924\n",
      "[Iter 8300 Task segm] Train Loss: 0.7960\n",
      "[Iter 8350 Task segm] Train Loss: 0.7982\n",
      "[Iter 8400 Task segm] Train Loss: 0.7819\n",
      "[Iter 8400 Task segm] Val Loss: 1.5140\n",
      "{'mIoU': 0.2349, 'Pixel Acc': 0.5613}\n",
      "======================================================================\n",
      "[Iter 8450 Task segm] Train Loss: 0.7720\n",
      "[Iter 8500 Task segm] Train Loss: 0.7841\n",
      "[Iter 8550 Task segm] Train Loss: 0.7641\n",
      "[Iter 8600 Task segm] Train Loss: 0.7935\n",
      "[Iter 8600 Task segm] Val Loss: 1.5001\n",
      "{'mIoU': 0.2398, 'Pixel Acc': 0.5658}\n",
      "======================================================================\n",
      "[Iter 8650 Task segm] Train Loss: 0.7762\n",
      "[Iter 8700 Task segm] Train Loss: 0.7672\n",
      "[Iter 8750 Task segm] Train Loss: 0.7761\n",
      "[Iter 8800 Task segm] Train Loss: 0.7666\n",
      "[Iter 8800 Task segm] Val Loss: 1.5306\n",
      "{'mIoU': 0.2417, 'Pixel Acc': 0.5591}\n",
      "======================================================================\n",
      "[Iter 8850 Task segm] Train Loss: 0.7777\n",
      "[Iter 8900 Task segm] Train Loss: 0.7411\n",
      "[Iter 8950 Task segm] Train Loss: 0.7570\n",
      "[Iter 9000 Task segm] Train Loss: 0.7382\n",
      "[Iter 9000 Task segm] Val Loss: 1.5313\n",
      "{'mIoU': 0.2414, 'Pixel Acc': 0.5598}\n",
      "======================================================================\n",
      "[Iter 9050 Task segm] Train Loss: 0.7674\n",
      "[Iter 9100 Task segm] Train Loss: 0.7285\n",
      "[Iter 9150 Task segm] Train Loss: 0.7155\n",
      "[Iter 9200 Task segm] Train Loss: 0.7219\n",
      "[Iter 9200 Task segm] Val Loss: 1.5311\n",
      "{'mIoU': 0.2505, 'Pixel Acc': 0.5574}\n",
      "======================================================================\n",
      "[Iter 9250 Task segm] Train Loss: 0.7122\n",
      "[Iter 9300 Task segm] Train Loss: 0.7222\n",
      "[Iter 9350 Task segm] Train Loss: 0.7515\n",
      "[Iter 9400 Task segm] Train Loss: 0.7367\n",
      "[Iter 9400 Task segm] Val Loss: 1.5122\n",
      "{'mIoU': 0.2392, 'Pixel Acc': 0.5656}\n",
      "======================================================================\n",
      "[Iter 9450 Task segm] Train Loss: 0.7315\n",
      "[Iter 9500 Task segm] Train Loss: 0.7065\n",
      "[Iter 9550 Task segm] Train Loss: 0.7216\n",
      "[Iter 9600 Task segm] Train Loss: 0.7277\n",
      "[Iter 9600 Task segm] Val Loss: 1.5932\n",
      "{'mIoU': 0.2309, 'Pixel Acc': 0.5547}\n",
      "======================================================================\n",
      "[Iter 9650 Task segm] Train Loss: 0.7189\n",
      "[Iter 9700 Task segm] Train Loss: 0.7068\n",
      "[Iter 9750 Task segm] Train Loss: 0.7012\n",
      "[Iter 9800 Task segm] Train Loss: 0.7149\n",
      "[Iter 9800 Task segm] Val Loss: 1.5821\n",
      "{'mIoU': 0.2345, 'Pixel Acc': 0.5534}\n",
      "======================================================================\n",
      "[Iter 9850 Task segm] Train Loss: 0.7122\n",
      "[Iter 9900 Task segm] Train Loss: 0.6911\n",
      "[Iter 9950 Task segm] Train Loss: 0.6598\n",
      "[Iter 10000 Task segm] Train Loss: 0.6927\n",
      "[Iter 10000 Task segm] Val Loss: 1.5381\n",
      "{'mIoU': 0.2424, 'Pixel Acc': 0.5658}\n",
      "======================================================================\n",
      "[Iter 10050 Task segm] Train Loss: 0.7055\n",
      "[Iter 10100 Task segm] Train Loss: 0.7051\n",
      "[Iter 10150 Task segm] Train Loss: 0.6887\n",
      "[Iter 10200 Task segm] Train Loss: 0.6803\n",
      "[Iter 10200 Task segm] Val Loss: 1.6002\n",
      "{'mIoU': 0.2303, 'Pixel Acc': 0.5595}\n",
      "======================================================================\n",
      "[Iter 10250 Task segm] Train Loss: 0.6759\n",
      "[Iter 10300 Task segm] Train Loss: 0.6709\n",
      "[Iter 10350 Task segm] Train Loss: 0.6574\n",
      "[Iter 10400 Task segm] Train Loss: 0.6478\n",
      "[Iter 10400 Task segm] Val Loss: 1.5610\n",
      "{'mIoU': 0.2489, 'Pixel Acc': 0.5616}\n",
      "======================================================================\n",
      "[Iter 10450 Task segm] Train Loss: 0.6458\n",
      "[Iter 10500 Task segm] Train Loss: 0.6523\n",
      "[Iter 10550 Task segm] Train Loss: 0.6416\n"
     ]
    }
   ],
   "source": [
    "# Ind. Model, Adam, task = Seg\n",
    "checkpoint = '/mnt/nfs/work1/huiguan/lijunzhang/multibranch/checkpoint/NYUv2/exp/'\n",
    "\n",
    "trainer = Trainer(model, task[0], trainDataloader, valDataloader, criterion, metric)\n",
    "trainer.train(20000, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 10450 Task segm] Train Loss: 0.6542\n",
      "[Iter 10500 Task segm] Train Loss: 0.6175\n",
      "[Iter 10550 Task segm] Train Loss: 0.6675\n",
      "[Iter 10600 Task segm] Train Loss: 0.6615\n",
      "[Iter 10600 Task segm] Val Loss: 1.5958\n",
      "{'mIoU': 0.2216, 'Pixel Acc': 0.5542}\n",
      "======================================================================\n",
      "[Iter 10650 Task segm] Train Loss: 0.6344\n",
      "[Iter 10700 Task segm] Train Loss: 0.6211\n",
      "[Iter 10750 Task segm] Train Loss: 0.6460\n",
      "[Iter 10800 Task segm] Train Loss: 0.6333\n",
      "[Iter 10800 Task segm] Val Loss: 1.5543\n",
      "{'mIoU': 0.2439, 'Pixel Acc': 0.5665}\n",
      "======================================================================\n",
      "[Iter 10850 Task segm] Train Loss: 0.6476\n",
      "[Iter 10900 Task segm] Train Loss: 0.6531\n",
      "[Iter 10950 Task segm] Train Loss: 0.6705\n",
      "[Iter 11000 Task segm] Train Loss: 0.6323\n",
      "[Iter 11000 Task segm] Val Loss: 1.5706\n",
      "{'mIoU': 0.2416, 'Pixel Acc': 0.5617}\n",
      "======================================================================\n",
      "[Iter 11050 Task segm] Train Loss: 0.6205\n",
      "[Iter 11100 Task segm] Train Loss: 0.6342\n",
      "[Iter 11150 Task segm] Train Loss: 0.6079\n",
      "[Iter 11200 Task segm] Train Loss: 0.6386\n",
      "[Iter 11200 Task segm] Val Loss: 1.5519\n",
      "{'mIoU': 0.2357, 'Pixel Acc': 0.5626}\n",
      "======================================================================\n",
      "[Iter 11250 Task segm] Train Loss: 0.5985\n",
      "[Iter 11300 Task segm] Train Loss: 0.6145\n",
      "[Iter 11350 Task segm] Train Loss: 0.6180\n",
      "[Iter 11400 Task segm] Train Loss: 0.6353\n",
      "[Iter 11400 Task segm] Val Loss: 1.5553\n",
      "{'mIoU': 0.2381, 'Pixel Acc': 0.5618}\n",
      "======================================================================\n",
      "[Iter 11450 Task segm] Train Loss: 0.6427\n",
      "[Iter 11500 Task segm] Train Loss: 0.6070\n",
      "[Iter 11550 Task segm] Train Loss: 0.6013\n",
      "[Iter 11600 Task segm] Train Loss: 0.6311\n",
      "[Iter 11600 Task segm] Val Loss: 1.6376\n",
      "{'mIoU': 0.2211, 'Pixel Acc': 0.5534}\n",
      "======================================================================\n",
      "[Iter 11650 Task segm] Train Loss: 0.5789\n",
      "[Iter 11700 Task segm] Train Loss: 0.5773\n",
      "[Iter 11750 Task segm] Train Loss: 0.6085\n",
      "[Iter 11800 Task segm] Train Loss: 0.5843\n",
      "[Iter 11800 Task segm] Val Loss: 1.5709\n",
      "{'mIoU': 0.2445, 'Pixel Acc': 0.5638}\n",
      "======================================================================\n",
      "[Iter 11850 Task segm] Train Loss: 0.5758\n",
      "[Iter 11900 Task segm] Train Loss: 0.5989\n",
      "[Iter 11950 Task segm] Train Loss: 0.5880\n",
      "[Iter 12000 Task segm] Train Loss: 0.5603\n",
      "[Iter 12000 Task segm] Val Loss: 1.5755\n",
      "{'mIoU': 0.2474, 'Pixel Acc': 0.5607}\n",
      "======================================================================\n",
      "[Iter 12050 Task segm] Train Loss: 0.5398\n",
      "[Iter 12100 Task segm] Train Loss: 0.5232\n",
      "[Iter 12150 Task segm] Train Loss: 0.5280\n",
      "[Iter 12200 Task segm] Train Loss: 0.5372\n",
      "[Iter 12200 Task segm] Val Loss: 1.5274\n",
      "{'mIoU': 0.2509, 'Pixel Acc': 0.5768}\n",
      "======================================================================\n",
      "[Iter 12250 Task segm] Train Loss: 0.5205\n",
      "[Iter 12300 Task segm] Train Loss: 0.4999\n",
      "[Iter 12350 Task segm] Train Loss: 0.4966\n",
      "[Iter 12400 Task segm] Train Loss: 0.5071\n",
      "[Iter 12400 Task segm] Val Loss: 1.5650\n",
      "{'mIoU': 0.2479, 'Pixel Acc': 0.5699}\n",
      "======================================================================\n",
      "[Iter 12450 Task segm] Train Loss: 0.5010\n",
      "[Iter 12500 Task segm] Train Loss: 0.5084\n",
      "[Iter 12550 Task segm] Train Loss: 0.5217\n",
      "[Iter 12600 Task segm] Train Loss: 0.5105\n",
      "[Iter 12600 Task segm] Val Loss: 1.5619\n",
      "{'mIoU': 0.2462, 'Pixel Acc': 0.5726}\n",
      "======================================================================\n",
      "[Iter 12650 Task segm] Train Loss: 0.5129\n",
      "[Iter 12700 Task segm] Train Loss: 0.4866\n",
      "[Iter 12750 Task segm] Train Loss: 0.5024\n",
      "[Iter 12800 Task segm] Train Loss: 0.4905\n",
      "[Iter 12800 Task segm] Val Loss: 1.6098\n",
      "{'mIoU': 0.2409, 'Pixel Acc': 0.5687}\n",
      "======================================================================\n",
      "[Iter 12850 Task segm] Train Loss: 0.4923\n",
      "[Iter 12900 Task segm] Train Loss: 0.4893\n",
      "[Iter 12950 Task segm] Train Loss: 0.4844\n",
      "[Iter 13000 Task segm] Train Loss: 0.4787\n",
      "[Iter 13000 Task segm] Val Loss: 1.6135\n",
      "{'mIoU': 0.2401, 'Pixel Acc': 0.5706}\n",
      "======================================================================\n",
      "[Iter 13050 Task segm] Train Loss: 0.5049\n",
      "[Iter 13100 Task segm] Train Loss: 0.4814\n",
      "[Iter 13150 Task segm] Train Loss: 0.4977\n",
      "[Iter 13200 Task segm] Train Loss: 0.4869\n",
      "[Iter 13200 Task segm] Val Loss: 1.5861\n",
      "{'mIoU': 0.249, 'Pixel Acc': 0.5743}\n",
      "======================================================================\n",
      "[Iter 13250 Task segm] Train Loss: 0.4614\n",
      "[Iter 13300 Task segm] Train Loss: 0.4724\n",
      "[Iter 13350 Task segm] Train Loss: 0.4735\n",
      "[Iter 13400 Task segm] Train Loss: 0.4795\n",
      "[Iter 13400 Task segm] Val Loss: 1.5936\n",
      "{'mIoU': 0.2438, 'Pixel Acc': 0.5709}\n",
      "======================================================================\n",
      "[Iter 13450 Task segm] Train Loss: 0.4831\n",
      "[Iter 13500 Task segm] Train Loss: 0.4987\n",
      "[Iter 13550 Task segm] Train Loss: 0.4669\n",
      "[Iter 13600 Task segm] Train Loss: 0.4772\n",
      "[Iter 13600 Task segm] Val Loss: 1.5847\n",
      "{'mIoU': 0.2509, 'Pixel Acc': 0.5735}\n",
      "======================================================================\n",
      "[Iter 13650 Task segm] Train Loss: 0.4608\n",
      "[Iter 13700 Task segm] Train Loss: 0.4727\n",
      "[Iter 13750 Task segm] Train Loss: 0.4718\n",
      "[Iter 13800 Task segm] Train Loss: 0.4682\n",
      "[Iter 13800 Task segm] Val Loss: 1.6084\n",
      "{'mIoU': 0.252, 'Pixel Acc': 0.5761}\n",
      "======================================================================\n",
      "[Iter 13850 Task segm] Train Loss: 0.4413\n",
      "[Iter 13900 Task segm] Train Loss: 0.4554\n",
      "[Iter 13950 Task segm] Train Loss: 0.4541\n",
      "[Iter 14000 Task segm] Train Loss: 0.4525\n",
      "[Iter 14000 Task segm] Val Loss: 1.5770\n",
      "{'mIoU': 0.2666, 'Pixel Acc': 0.5754}\n",
      "======================================================================\n",
      "[Iter 14050 Task segm] Train Loss: 0.4605\n",
      "[Iter 14100 Task segm] Train Loss: 0.4528\n",
      "[Iter 14150 Task segm] Train Loss: 0.4627\n",
      "[Iter 14200 Task segm] Train Loss: 0.4454\n",
      "[Iter 14200 Task segm] Val Loss: 1.5816\n",
      "{'mIoU': 0.2547, 'Pixel Acc': 0.5758}\n",
      "======================================================================\n",
      "[Iter 14250 Task segm] Train Loss: 0.4479\n",
      "[Iter 14300 Task segm] Train Loss: 0.4366\n",
      "[Iter 14350 Task segm] Train Loss: 0.4493\n",
      "[Iter 14400 Task segm] Train Loss: 0.4487\n",
      "[Iter 14400 Task segm] Val Loss: 1.6298\n",
      "{'mIoU': 0.2458, 'Pixel Acc': 0.5711}\n",
      "======================================================================\n",
      "[Iter 14450 Task segm] Train Loss: 0.4639\n",
      "[Iter 14500 Task segm] Train Loss: 0.4406\n",
      "[Iter 14550 Task segm] Train Loss: 0.4332\n",
      "[Iter 14600 Task segm] Train Loss: 0.4554\n",
      "[Iter 14600 Task segm] Val Loss: 1.7228\n",
      "{'mIoU': 0.2386, 'Pixel Acc': 0.5645}\n",
      "======================================================================\n",
      "[Iter 14650 Task segm] Train Loss: 0.4531\n",
      "[Iter 14700 Task segm] Train Loss: 0.4391\n",
      "[Iter 14750 Task segm] Train Loss: 0.4705\n",
      "[Iter 14800 Task segm] Train Loss: 0.4511\n",
      "[Iter 14800 Task segm] Val Loss: 1.5641\n",
      "{'mIoU': 0.2575, 'Pixel Acc': 0.5804}\n",
      "======================================================================\n",
      "[Iter 14850 Task segm] Train Loss: 0.4319\n",
      "[Iter 14900 Task segm] Train Loss: 0.4356\n",
      "[Iter 14950 Task segm] Train Loss: 0.4425\n",
      "[Iter 15000 Task segm] Train Loss: 0.4274\n",
      "[Iter 15000 Task segm] Val Loss: 1.6346\n",
      "{'mIoU': 0.246, 'Pixel Acc': 0.5739}\n",
      "======================================================================\n",
      "[Iter 15050 Task segm] Train Loss: 0.4425\n",
      "[Iter 15100 Task segm] Train Loss: 0.4341\n",
      "[Iter 15150 Task segm] Train Loss: 0.4252\n",
      "[Iter 15200 Task segm] Train Loss: 0.4259\n",
      "[Iter 15200 Task segm] Val Loss: 1.6783\n",
      "{'mIoU': 0.241, 'Pixel Acc': 0.5667}\n",
      "======================================================================\n",
      "[Iter 15250 Task segm] Train Loss: 0.4301\n",
      "[Iter 15300 Task segm] Train Loss: 0.4222\n",
      "[Iter 15350 Task segm] Train Loss: 0.3978\n",
      "[Iter 15400 Task segm] Train Loss: 0.4379\n",
      "[Iter 15400 Task segm] Val Loss: 1.6416\n",
      "{'mIoU': 0.248, 'Pixel Acc': 0.5716}\n",
      "======================================================================\n",
      "[Iter 15450 Task segm] Train Loss: 0.4195\n",
      "[Iter 15500 Task segm] Train Loss: 0.4278\n",
      "[Iter 15550 Task segm] Train Loss: 0.4360\n",
      "[Iter 15600 Task segm] Train Loss: 0.4448\n",
      "[Iter 15600 Task segm] Val Loss: 1.6351\n",
      "{'mIoU': 0.2445, 'Pixel Acc': 0.5685}\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 15650 Task segm] Train Loss: 0.4414\n",
      "[Iter 15700 Task segm] Train Loss: 0.4217\n",
      "[Iter 15750 Task segm] Train Loss: 0.4452\n",
      "[Iter 15800 Task segm] Train Loss: 0.4212\n",
      "[Iter 15800 Task segm] Val Loss: 1.6450\n",
      "{'mIoU': 0.2452, 'Pixel Acc': 0.5702}\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c791990837c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalDataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'segment_semantic.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-c31ec4949d61>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, iters, savePath, reload)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartIter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_iters\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c31ec4949d61>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/multitask/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/multitask/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/multitask/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/multitask/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/multibranch/data/nyuv2_dataloader_adashare.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mnormal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ind. Model, Adam, task = Seg, reload\n",
    "checkpoint = '/mnt/nfs/work1/huiguan/lijunzhang/multibranch/checkpoint/NYUv2/exp/'\n",
    "\n",
    "trainer = Trainer(model, task[0], trainDataloader, valDataloader, criterion, metric)\n",
    "trainer.train(20000, checkpoint, reload='segment_semantic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 15850 Task segm] Train Loss: 0.4104\n",
      "[Iter 15900 Task segm] Train Loss: 0.4212\n",
      "[Iter 15950 Task segm] Train Loss: 0.4096\n",
      "[Iter 16000 Task segm] Train Loss: 0.4025\n",
      "[Iter 16000 Task segm] Val Loss: 1.6363\n",
      "{'mIoU': 0.2494, 'Pixel Acc': 0.5732}\n",
      "======================================================================\n",
      "[Iter 16050 Task segm] Train Loss: 0.3969\n",
      "[Iter 16100 Task segm] Train Loss: 0.3784\n",
      "[Iter 16150 Task segm] Train Loss: 0.3863\n",
      "[Iter 16200 Task segm] Train Loss: 0.3849\n",
      "[Iter 16200 Task segm] Val Loss: 1.6353\n",
      "{'mIoU': 0.251, 'Pixel Acc': 0.5779}\n",
      "======================================================================\n",
      "[Iter 16250 Task segm] Train Loss: 0.4020\n",
      "[Iter 16300 Task segm] Train Loss: 0.3770\n",
      "[Iter 16350 Task segm] Train Loss: 0.3888\n",
      "[Iter 16400 Task segm] Train Loss: 0.3816\n",
      "[Iter 16400 Task segm] Val Loss: 1.6435\n",
      "{'mIoU': 0.2529, 'Pixel Acc': 0.5773}\n",
      "======================================================================\n",
      "[Iter 16450 Task segm] Train Loss: 0.3742\n",
      "[Iter 16500 Task segm] Train Loss: 0.3705\n",
      "[Iter 16550 Task segm] Train Loss: 0.3665\n",
      "[Iter 16600 Task segm] Train Loss: 0.3666\n",
      "[Iter 16600 Task segm] Val Loss: 1.6503\n",
      "{'mIoU': 0.2513, 'Pixel Acc': 0.5781}\n",
      "======================================================================\n",
      "[Iter 16650 Task segm] Train Loss: 0.3642\n",
      "[Iter 16700 Task segm] Train Loss: 0.3665\n",
      "[Iter 16750 Task segm] Train Loss: 0.3667\n",
      "[Iter 16800 Task segm] Train Loss: 0.3601\n",
      "[Iter 16800 Task segm] Val Loss: 1.6368\n",
      "{'mIoU': 0.2568, 'Pixel Acc': 0.5799}\n",
      "======================================================================\n",
      "[Iter 16850 Task segm] Train Loss: 0.3736\n",
      "[Iter 16900 Task segm] Train Loss: 0.3594\n",
      "[Iter 16950 Task segm] Train Loss: 0.3576\n",
      "[Iter 17000 Task segm] Train Loss: 0.3629\n",
      "[Iter 17000 Task segm] Val Loss: 1.6352\n",
      "{'mIoU': 0.251, 'Pixel Acc': 0.578}\n",
      "======================================================================\n",
      "[Iter 17050 Task segm] Train Loss: 0.3620\n",
      "[Iter 17100 Task segm] Train Loss: 0.3664\n",
      "[Iter 17150 Task segm] Train Loss: 0.3742\n",
      "[Iter 17200 Task segm] Train Loss: 0.3550\n",
      "[Iter 17200 Task segm] Val Loss: 1.6180\n",
      "{'mIoU': 0.2581, 'Pixel Acc': 0.5836}\n",
      "======================================================================\n",
      "[Iter 17250 Task segm] Train Loss: 0.3586\n",
      "[Iter 17300 Task segm] Train Loss: 0.3494\n",
      "[Iter 17350 Task segm] Train Loss: 0.3488\n",
      "[Iter 17400 Task segm] Train Loss: 0.3704\n",
      "[Iter 17400 Task segm] Val Loss: 1.6378\n",
      "{'mIoU': 0.2568, 'Pixel Acc': 0.58}\n",
      "======================================================================\n",
      "[Iter 17450 Task segm] Train Loss: 0.3586\n",
      "[Iter 17500 Task segm] Train Loss: 0.3656\n",
      "[Iter 17550 Task segm] Train Loss: 0.3542\n",
      "[Iter 17600 Task segm] Train Loss: 0.3595\n",
      "[Iter 17600 Task segm] Val Loss: 1.6457\n",
      "{'mIoU': 0.2524, 'Pixel Acc': 0.5782}\n",
      "======================================================================\n",
      "[Iter 17650 Task segm] Train Loss: 0.3444\n",
      "[Iter 17700 Task segm] Train Loss: 0.3579\n",
      "[Iter 17750 Task segm] Train Loss: 0.3456\n",
      "[Iter 17800 Task segm] Train Loss: 0.3473\n",
      "[Iter 17800 Task segm] Val Loss: 1.6725\n",
      "{'mIoU': 0.2522, 'Pixel Acc': 0.573}\n",
      "======================================================================\n",
      "[Iter 17850 Task segm] Train Loss: 0.3351\n",
      "[Iter 17900 Task segm] Train Loss: 0.3472\n",
      "[Iter 17950 Task segm] Train Loss: 0.3528\n",
      "[Iter 18000 Task segm] Train Loss: 0.3466\n",
      "[Iter 18000 Task segm] Val Loss: 1.6573\n",
      "{'mIoU': 0.2567, 'Pixel Acc': 0.5798}\n",
      "======================================================================\n",
      "[Iter 18050 Task segm] Train Loss: 0.3481\n",
      "[Iter 18100 Task segm] Train Loss: 0.3597\n",
      "[Iter 18150 Task segm] Train Loss: 0.3557\n",
      "[Iter 18200 Task segm] Train Loss: 0.3356\n",
      "[Iter 18200 Task segm] Val Loss: 1.6344\n",
      "{'mIoU': 0.2575, 'Pixel Acc': 0.5837}\n",
      "======================================================================\n",
      "[Iter 18250 Task segm] Train Loss: 0.3497\n",
      "[Iter 18300 Task segm] Train Loss: 0.3383\n",
      "[Iter 18350 Task segm] Train Loss: 0.3529\n",
      "[Iter 18400 Task segm] Train Loss: 0.3492\n",
      "[Iter 18400 Task segm] Val Loss: 1.6432\n",
      "{'mIoU': 0.2526, 'Pixel Acc': 0.5808}\n",
      "======================================================================\n",
      "[Iter 18450 Task segm] Train Loss: 0.3652\n",
      "[Iter 18500 Task segm] Train Loss: 0.3412\n",
      "[Iter 18550 Task segm] Train Loss: 0.3471\n",
      "[Iter 18600 Task segm] Train Loss: 0.3445\n",
      "[Iter 18600 Task segm] Val Loss: 1.6218\n",
      "{'mIoU': 0.2617, 'Pixel Acc': 0.5833}\n",
      "======================================================================\n",
      "[Iter 18650 Task segm] Train Loss: 0.3516\n",
      "[Iter 18700 Task segm] Train Loss: 0.3465\n",
      "[Iter 18750 Task segm] Train Loss: 0.3477\n",
      "[Iter 18800 Task segm] Train Loss: 0.3362\n",
      "[Iter 18800 Task segm] Val Loss: 1.6527\n",
      "{'mIoU': 0.2558, 'Pixel Acc': 0.5788}\n",
      "======================================================================\n",
      "[Iter 18850 Task segm] Train Loss: 0.3503\n",
      "[Iter 18900 Task segm] Train Loss: 0.3338\n",
      "[Iter 18950 Task segm] Train Loss: 0.3301\n",
      "[Iter 19000 Task segm] Train Loss: 0.3470\n",
      "[Iter 19000 Task segm] Val Loss: 1.6628\n",
      "{'mIoU': 0.2565, 'Pixel Acc': 0.5791}\n",
      "======================================================================\n",
      "[Iter 19050 Task segm] Train Loss: 0.3398\n",
      "[Iter 19100 Task segm] Train Loss: 0.3310\n",
      "[Iter 19150 Task segm] Train Loss: 0.3391\n",
      "[Iter 19200 Task segm] Train Loss: 0.3327\n",
      "[Iter 19200 Task segm] Val Loss: 1.6591\n",
      "{'mIoU': 0.2604, 'Pixel Acc': 0.5815}\n",
      "======================================================================\n",
      "[Iter 19250 Task segm] Train Loss: 0.3389\n",
      "[Iter 19300 Task segm] Train Loss: 0.3226\n",
      "[Iter 19350 Task segm] Train Loss: 0.3583\n",
      "[Iter 19400 Task segm] Train Loss: 0.3348\n",
      "[Iter 19400 Task segm] Val Loss: 1.6992\n",
      "{'mIoU': 0.2569, 'Pixel Acc': 0.578}\n",
      "======================================================================\n",
      "[Iter 19450 Task segm] Train Loss: 0.3379\n",
      "[Iter 19500 Task segm] Train Loss: 0.3286\n",
      "[Iter 19550 Task segm] Train Loss: 0.3282\n",
      "[Iter 19600 Task segm] Train Loss: 0.3285\n",
      "[Iter 19600 Task segm] Val Loss: 1.7098\n",
      "{'mIoU': 0.2544, 'Pixel Acc': 0.5742}\n",
      "======================================================================\n",
      "[Iter 19650 Task segm] Train Loss: 0.3225\n",
      "[Iter 19700 Task segm] Train Loss: 0.3434\n",
      "[Iter 19750 Task segm] Train Loss: 0.3296\n",
      "[Iter 19800 Task segm] Train Loss: 0.3326\n",
      "[Iter 19800 Task segm] Val Loss: 1.6929\n",
      "{'mIoU': 0.2577, 'Pixel Acc': 0.5761}\n",
      "======================================================================\n",
      "[Iter 19850 Task segm] Train Loss: 0.3265\n",
      "[Iter 19900 Task segm] Train Loss: 0.3406\n",
      "[Iter 19950 Task segm] Train Loss: 0.3357\n",
      "[Iter 20000 Task segm] Train Loss: 0.3285\n",
      "[Iter 20000 Task segm] Val Loss: 1.6890\n",
      "{'mIoU': 0.2578, 'Pixel Acc': 0.5795}\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Ind. Model, Adam, task = Seg, reload\n",
    "checkpoint = '/mnt/nfs/work1/huiguan/lijunzhang/multibranch/checkpoint/NYUv2/exp/'\n",
    "\n",
    "trainer = Trainer(model, task[0], trainDataloader, valDataloader, criterion, metric)\n",
    "trainer.train(20000, checkpoint, reload='segment_semantic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Deeplab_ASPP(\n",
       "  (backbone): Deeplab_ResNet_Backbone(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (block): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (block): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (block): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): ResidualBlock(\n",
       "        (block): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (ds): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): ResidualBlock(\n",
       "        (block): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): ResidualBlock(\n",
       "        (block): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): ResidualBlock(\n",
       "        (block): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (8): ResidualBlock(\n",
       "        (block): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (ds): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (9): ResidualBlock(\n",
       "        (block): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (10): ResidualBlock(\n",
       "        (block): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (11): ResidualBlock(\n",
       "        (block): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (12): ResidualBlock(\n",
       "        (block): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (13): ResidualBlock(\n",
       "        (block): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (14): ResidualBlock(\n",
       "        (block): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (ds): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (15): ResidualBlock(\n",
       "        (block): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (16): ResidualBlock(\n",
       "        (block): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (heads): ASPPHeadNode(\n",
       "    (fc1): Classification_Module(\n",
       "      (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv3): Conv2d(1024, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (fc2): Classification_Module(\n",
       "      (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12))\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv3): Conv2d(1024, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (fc3): Classification_Module(\n",
       "      (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18))\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv3): Conv2d(1024, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (fc4): Classification_Module(\n",
       "      (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24))\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv3): Conv2d(1024, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dot(var, params=None):\n",
    "    if params is not None:\n",
    "        assert isinstance(params.values()[0], Variable)\n",
    "        param_map = {id(v): k for k, v in params.items()}\n",
    "\n",
    "    node_attr = dict(style=\"filled\", shape=\"box\", align=\"left\", fontsize=\"12\", ranksep=\"0.1\", height=\"0.2\")\n",
    "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "    seen = set()\n",
    "\n",
    "    def size_to_str(size):\n",
    "        return \"(\" + (\", \").join([\"%d\" % v for v in size]) + \")\"\n",
    "\n",
    "    def add_nodes(var):\n",
    "        if var not in seen:\n",
    "            if torch.is_tensor(var):\n",
    "                dot.node(str(id(var)), size_to_str(var.size()), fillcolor=\"orange\")\n",
    "                dot.edge(str(id(var.grad_fn)), str(id(var)))\n",
    "                var = var.grad_fn\n",
    "            if hasattr(var, \"variable\"):\n",
    "                u = var.variable\n",
    "                name = param_map[id(u)] if params is not None else \"\"\n",
    "                node_name = \"%s\\n %s\" % (name, size_to_str(u.size()))\n",
    "#                 print(node_name)\n",
    "                \n",
    "                dot.node(str(id(var)), node_name, fillcolor=\"lightblue\")\n",
    "            else:\n",
    "                print(type(var).__name__)\n",
    "                \n",
    "                dot.node(str(id(var)), str(type(var).__name__))\n",
    "            seen.add(var)\n",
    "            if hasattr(var, \"next_functions\"):\n",
    "                for u in var.next_functions:\n",
    "                    if u[0] is not None:\n",
    "                        dot.edge(str(id(u[0])), str(id(var)))\n",
    "                        add_nodes(u[0])\n",
    "            if hasattr(var, \"saved_tensors\"):\n",
    "                for t in var.saved_tensors:\n",
    "                    dot.edge(str(id(t)), str(id(var)))\n",
    "                    add_nodes(t)\n",
    "\n",
    "    add_nodes(var)\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReluBackward0\n",
      "AddBackward0\n",
      "ReluBackward0\n",
      "AddBackward0\n",
      "ReluBackward0\n",
      "AddBackward0\n",
      "NativeBatchNormBackward\n",
      "MkldnnConvolutionBackward\n",
      "ReluBackward0\n",
      "AddBackward0\n",
      "ReluBackward0\n",
      "AddBackward0\n",
      "ReluBackward0\n",
      "AddBackward0\n",
      "ReluBackward0\n",
      "AddBackward0\n",
      "ReluBackward0\n",
      "AddBackward0\n",
      "ReluBackward0\n",
      "AddBackward0\n",
      "NativeBatchNormBackward\n",
      "MkldnnConvolutionBackward\n",
      "ReluBackward0\n",
      "AddBackward0\n",
      "ReluBackward0\n",
      "AddBackward0\n",
      "ReluBackward0\n",
      "AddBackward0\n",
      "ReluBackward0\n",
      "AddBackward0\n",
      "NativeBatchNormBackward\n",
      "MkldnnConvolutionBackward\n",
      "ReluBackward0\n",
      "AddBackward0\n",
      "ReluBackward0\n",
      "AddBackward0\n",
      "ReluBackward0\n",
      "AddBackward0\n",
      "MaxPool2DWithIndicesBackward\n",
      "ReluBackward1\n",
      "NativeBatchNormBackward\n",
      "MkldnnConvolutionBackward\n",
      "NativeBatchNormBackward\n",
      "MkldnnConvolutionBackward\n",
      "ReluBackward1\n",
      "NativeBatchNormBackward\n",
      "MkldnnConvolutionBackward\n",
      "NativeBatchNormBackward\n",
      "MkldnnConvolutionBackward\n",
      "ReluBackward1\n",
      "NativeBatchNormBackward\n",
      "MkldnnConvolutionBackward\n",
      "NativeBatchNormBackward\n",
      "MkldnnConvolutionBackward\n",
      "ReluBackward1\n",
      "NativeBatchNormBackward\n",
      "MkldnnConvolutionBackward\n",
      "NativeBatchNormBackward\n",
      "MkldnnConvolutionBackward\n",
      "ReluBackward1\n",
      "NativeBatchNormBackward\n",
      "MkldnnConvolutionBackward\n",
      "NativeBatchNormBackward\n",
      "MkldnnConvolutionBackward\n",
      "ReluBackward1\n",
      "NativeBatchNormBackward\n",
      "MkldnnConvolutionBackward\n",
      "NativeBatchNormBackward\n",
      "MkldnnConvolutionBackward\n",
      "ReluBackward1\n",
      "NativeBatchNormBackward\n",
      "MkldnnConvolutionBackward\n",
      "NativeBatchNormBackward\n",
      "MkldnnConvolutionBackward\n",
      "ReluBackward1\n",
      "NativeBatchNormBackward\n",
      "MkldnnConvolutionBackward\n",
      "NativeBatchNormBackward\n",
      "SlowConvDilated2DBackward\n",
      "ReluBackward1\n",
      "NativeBatchNormBackward\n",
      "SlowConvDilated2DBackward\n",
      "NativeBatchNormBackward\n",
      "SlowConvDilated2DBackward\n",
      "ReluBackward1\n",
      "NativeBatchNormBackward\n",
      "SlowConvDilated2DBackward\n",
      "NativeBatchNormBackward\n",
      "SlowConvDilated2DBackward\n",
      "ReluBackward1\n",
      "NativeBatchNormBackward\n",
      "SlowConvDilated2DBackward\n",
      "NativeBatchNormBackward\n",
      "SlowConvDilated2DBackward\n",
      "ReluBackward1\n",
      "NativeBatchNormBackward\n",
      "SlowConvDilated2DBackward\n",
      "NativeBatchNormBackward\n",
      "SlowConvDilated2DBackward\n",
      "ReluBackward1\n",
      "NativeBatchNormBackward\n",
      "SlowConvDilated2DBackward\n",
      "NativeBatchNormBackward\n",
      "SlowConvDilated2DBackward\n",
      "ReluBackward1\n",
      "NativeBatchNormBackward\n",
      "SlowConvDilated2DBackward\n",
      "NativeBatchNormBackward\n",
      "SlowConvDilated2DBackward\n",
      "ReluBackward1\n",
      "NativeBatchNormBackward\n",
      "SlowConvDilated2DBackward\n",
      "NativeBatchNormBackward\n",
      "SlowConvDilated2DBackward\n",
      "ReluBackward1\n",
      "NativeBatchNormBackward\n",
      "SlowConvDilated2DBackward\n",
      "NativeBatchNormBackward\n",
      "SlowConvDilated2DBackward\n",
      "ReluBackward1\n",
      "NativeBatchNormBackward\n",
      "SlowConvDilated2DBackward\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Digraph.gv.pdf'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.randn(1, 3, 224, 224)\n",
    "y = backbone(inputs)\n",
    "g = make_dot(y)\n",
    "g.view()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
