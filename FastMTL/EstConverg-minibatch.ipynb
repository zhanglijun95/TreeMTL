{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "diverse-velvet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from pathlib import Path\n",
    "import random\n",
    "from scipy import stats\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from collections import OrderedDict\n",
    "from ptflops import get_model_complexity_info\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from framework.layer_node import Conv2dNode, InputNode\n",
    "from main.layout import Layout\n",
    "from main.algorithms import enum_layout_wo_rdt, init_S, coarse_to_fined\n",
    "from main.auto_models import MTSeqBackbone, MTSeqModel, ComputeBlock\n",
    "from main.head import ASPPHeadNode\n",
    "from main.trainer import Trainer\n",
    "from main.algs_FMTL import simple_alignment, complex_alignment\n",
    "\n",
    "from data.nyuv2_dataloader_adashare import NYU_v2\n",
    "from data.pixel2pixel_loss import NYUCriterions\n",
    "from data.pixel2pixel_metrics import NYUMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "psychological-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0473c357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj, name):\n",
    "    with open('./exp/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('./exp/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-clerk",
   "metadata": {},
   "source": [
    "# backbone and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "married-arbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backbone\n",
    "# mobilenet\n",
    "backbone_type = 'mobilenet'\n",
    "prototxt = '../models/mobilenetv2.prototxt'\n",
    "D = coarse_B = 5\n",
    "mapping = {0:[0,1,2,3,4,5,6], 1:[7,8,9,10,11,12,13,14,15,16,17], 2:[18,19,20,21,22], \n",
    "           3:[23,24,25,26,27,28,29,30], 4:[31], 5:[32]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "generous-syracuse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_semantic\n",
      "normal\n",
      "depth_zbuffer\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "# NYUv2\n",
    "data = 'NYUv2'\n",
    "dataroot = '/mnt/nfs/work1/huiguan/lijunzhang/policymtl/data/NYUv2/'\n",
    "tasks = ['segment_semantic', 'normal', 'depth_zbuffer']\n",
    "cls_num = {'segment_semantic': 40, 'normal':3, 'depth_zbuffer': 1}\n",
    "\n",
    "dataset = NYU_v2(dataroot, 'train', crop_h=321, crop_w=321)\n",
    "trainDataloader = DataLoader(dataset, 32, shuffle=True)\n",
    "\n",
    "criterionDict = {}\n",
    "metricDict = {}\n",
    "for task in tasks:\n",
    "    print(task, flush=True)\n",
    "    criterionDict[task] = NYUCriterions(task)\n",
    "    metricDict[task] = NYUMetrics(task)\n",
    "\n",
    "input_dim = (3,321,321)\n",
    "T = len(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f52ba25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix dataloader for fix mini-batches\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "trainDataloaderFix = DataLoader(dataset, 96, shuffle=True,worker_init_fn=seed_worker,generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2f205c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainDataloaderFix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cloudy-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind. weights\n",
    "ckpt_PATH = '/mnt/nfs/work1/huiguan/lijunzhang/multibranch/checkpoint/'\n",
    "weight_PATH = ckpt_PATH + 'NYUv2/ind/mobilenet/segment_semantic_normal_depth_zbuffer.model' # NYUv2 + MobileNetV2, from the same init\n",
    "# weight_PATH = ckpt_PATH + 'NYUv2/baseline/WPreMobile/2/segment_semantic_normal_depth_zbuffer.model' # NYUv2 + MobileNetV2, from the same init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-veteran",
   "metadata": {},
   "source": [
    "# load independent model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "greek-spectacular",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    backbone = MTSeqBackbone(prototxt)\n",
    "    fined_B = len(backbone.basic_blocks)\n",
    "    feature_dim = backbone(torch.rand(1,3,224,224)).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sunset-mechanism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ind. Layout:\n",
      "[[{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}]]\n",
      "Construct MTSeqModel from Layout:\n",
      "[[{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}], [{0}, {1}, {2}]]\n"
     ]
    }
   ],
   "source": [
    "# ind. layout\n",
    "S = []\n",
    "for i in range(fined_B):\n",
    "    S.append([set([x]) for x in range(T)])\n",
    "layout = Layout(T, fined_B, S) \n",
    "print('Ind. Layout:', flush=True)\n",
    "print(layout, flush=True)\n",
    "\n",
    "# model\n",
    "with torch.no_grad():\n",
    "    model = MTSeqModel(prototxt, layout=layout, feature_dim=feature_dim, cls_num=cls_num)\n",
    "#     model = model.cuda()\n",
    "\n",
    "    # load ind. model weights\n",
    "    model.load_state_dict(torch.load(weight_PATH)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd92834e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-18283815aae0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#     x = data['input'].cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/multitask/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/multibranch/main/auto_models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/multitask/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/multibranch/main/auto_models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmtl_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/multibranch/main/auto_models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/multibranch/framework/base_node.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minternal_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/multibranch/framework/base_node.py\u001b[0m in \u001b[0;36minternal_compute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# Function: Forward function when commonly used (shared with other tasks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m#           [default for layers with 1 bottom]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasicOp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfatherNodeList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/multitask/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/multitask/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/multitask/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "# compute r0 --> stop convergency loss for layout convergency iter estimation\n",
    "loss_lst = {task:[] for task in tasks}\n",
    "# model = model.cuda()\n",
    "model.train()\n",
    "for i, data in enumerate(trainDataloader):\n",
    "    if i > 50:\n",
    "        break\n",
    "    x = data['input'].cuda()\n",
    "    output = model(x)\n",
    "    for task in tasks:\n",
    "        y = data[task].cuda()\n",
    "        if task + '_mask' in data:\n",
    "            tloss = criterionDict[task](output[task], y, data[task + '_mask'].cuda())\n",
    "        else:\n",
    "            tloss = criterionDict[task](output[task], y)\n",
    "        loss_lst[task].append(tloss.item())\n",
    "        print('{}: {:.4f}'.format(task,tloss.item()))\n",
    "    print('-'*30)\n",
    "\n",
    "target = {task: np.mean(loss_lst[task]) for task in tasks}\n",
    "print('r0: {}'.format(target))\n",
    "# save_obj(target, 'r0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "849f5499",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = load_obj('r0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1ee8ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'segment_semantic': 0.7207628560066223,\n",
       " 'normal': 0.07310347080230713,\n",
       " 'depth_zbuffer': 1.1167707586288451}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8672a0",
   "metadata": {},
   "source": [
    "# enum layouts and channel alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c03889d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enum layout\n",
    "layout_list = [] \n",
    "S0 = init_S(T, coarse_B) # initial state\n",
    "L = Layout(T, coarse_B, S0) # initial layout\n",
    "layout_list.append(L)\n",
    "enum_layout_wo_rdt(L, layout_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e15284aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "align_choice = 2 # 0: no align; 1: simple align (use out_ord only); 2: complex align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3c90818",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "if align_choice == 1:\n",
    "    simple_alignment(model, tasks)\n",
    "elif align_choice == 2:\n",
    "    complex_alignment(model, tasks)\n",
    "elif align_choice == 0:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49848dc0",
   "metadata": {},
   "source": [
    "# est. convergence rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8241f4e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layout 0\n",
      "Fined Layout:\n",
      "[[{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}], [{0, 1, 2}]]\n",
      "Task segment_semantic:\n",
      "\t\tLoss Samples: [3.2886670446395874, 2.958666591644287, 2.836396155357361, 2.7248942041397095]\n",
      "\t\tAlpha: 0.09285630247136648\n",
      "\t\tEst Iter: 1100\n",
      "\t\tFinal Loss: -41.23616573052018\n",
      "Task normal:\n",
      "\t\tLoss Samples: [0.1140168035030365, 0.08978825688362121, 0.0878688395023346, 0.08668279767036438]\n",
      "\t\tAlpha: 0.1898632418534569\n",
      "\t\tEst Iter: 1450\n",
      "\t\tFinal Loss: -0.33502806546669794\n",
      "Task depth_zbuffer:\n",
      "\t\tLoss Samples: [1.3181030833721161, 0.9099568724632263, 0.6679672265052795, 0.4986503142118454]\n",
      "\t\tAlpha: 0.6831871100992039\n",
      "\t\tEst Iter: 100\n",
      "\t\tFinal Loss: -30.853085296054104\n",
      "--------------------------------------------------------------------------------\n",
      "Task segment_semantic:\n",
      "\t\tLoss Samples: [3.2343884038925172, 2.8118711709976196, 2.66775360584259, 2.5408286905288695]\n",
      "\t\tAlpha: 0.1181048698622647\n",
      "\t\tEst Iter: 900\n",
      "\t\tFinal Loss: -47.12343253710049\n",
      "Task normal:\n",
      "\t\tLoss Samples: [0.08822696805000305, 0.07174246191978455, 0.0705855119228363, 0.06955768704414368]\n",
      "\t\tAlpha: 0.04454600907592961\n",
      "\t\tEst Iter: 650\n",
      "\t\tFinal Loss: -0.3372660401309053\n",
      "Task depth_zbuffer:\n",
      "\t\tLoss Samples: [1.2525737762451172, 0.876322751045227, 0.6251074182987213, 0.45109411954879763]\n",
      "\t\tAlpha: 0.9089796188519182\n",
      "\t\tEst Iter: 100\n",
      "\t\tFinal Loss: -1.8788210677314787\n",
      "--------------------------------------------------------------------------------\n",
      "Task segment_semantic:\n",
      "\t\tLoss Samples: [3.2209636259078978, 2.7642584228515625, 2.5827064323425293, 2.435614676475525]\n",
      "\t\tAlpha: 0.22816951888433676\n",
      "\t\tEst Iter: 800\n",
      "\t\tFinal Loss: -52.577731026712314\n",
      "Task normal:\n",
      "\t\tLoss Samples: [0.08790927290916443, 0.06932796001434326, 0.06783423900604248, 0.06646429657936097]\n",
      "\t\tAlpha: 0.03431397174679235\n",
      "\t\tEst Iter: 400\n",
      "\t\tFinal Loss: -0.4770996247173724\n",
      "Task depth_zbuffer:\n",
      "\t\tLoss Samples: [1.1881305158138276, 0.8214970886707306, 0.6203016018867493, 0.45319394528865814]\n",
      "\t\tAlpha: 0.3093538329672562\n",
      "\t\tEst Iter: 100\n",
      "\t\tFinal Loss: -60.75482940520847\n",
      "--------------------------------------------------------------------------------\n",
      "Task segment_semantic:\n",
      "\t\tLoss Samples: [3.211610321998596, 2.797337517738342, 2.626560673713684, 2.469129424095154]\n",
      "\t\tAlpha: 0.09182111881757515\n",
      "\t\tEst Iter: 750\n",
      "\t\tFinal Loss: -59.67528241546444\n",
      "Task normal:\n",
      "\t\tLoss Samples: [0.08399689197540283, 0.07017277598381043, 0.06803026556968689, 0.06619471907615662]\n",
      "\t\tAlpha: 0.08293979932001985\n",
      "\t\tEst Iter: 350\n",
      "\t\tFinal Loss: -0.6542091711822309\n",
      "Task depth_zbuffer:\n",
      "\t\tLoss Samples: [1.002561744451523, 0.7188599276542663, 0.5494143867492676, 0.4272501426935196]\n",
      "\t\tAlpha: 0.6347888150335436\n",
      "\t\tEst Iter: 100\n",
      "\t\tFinal Loss: -27.182974419455284\n",
      "--------------------------------------------------------------------------------\n",
      "Task segment_semantic:\n",
      "\t\tLoss Samples: [3.2749428749084473, 2.831523609161377, 2.6773927450180053, 2.5552940940856934]\n",
      "\t\tAlpha: 0.2204690525661907\n",
      "\t\tEst Iter: 1000\n",
      "\t\tFinal Loss: -42.943380530149675\n",
      "Task normal:\n",
      "\t\tLoss Samples: [0.08300994157791138, 0.06896817922592163, 0.06721919894218445, 0.06597686886787414]\n",
      "\t\tAlpha: 0.16420726038860747\n",
      "\t\tEst Iter: 450\n",
      "\t\tFinal Loss: -0.39635039408743733\n",
      "Task depth_zbuffer:\n",
      "\t\tLoss Samples: [1.0979587709903718, 0.750695505142212, 0.5316019266843796, 0.4132935893535614]\n",
      "\t\tAlpha: 1.3378761823994743\n",
      "\t\tEst Iter: 100\n",
      "\t\tFinal Loss: 0.3396748151485941\n",
      "--------------------------------------------------------------------------------\n",
      "Task segment_semantic:\n",
      "\t\tLoss Samples: [3.2775207138061524, 2.845722632408142, 2.669605736732483, 2.531198959350586]\n",
      "\t\tAlpha: 0.26867553966300944\n",
      "\t\tEst Iter: 900\n",
      "\t\tFinal Loss: -47.89219060377765\n",
      "Task normal:\n",
      "\t\tLoss Samples: False\n",
      "\t\tBad Loss Samples\n",
      "Task depth_zbuffer:\n",
      "\t\tLoss Samples: [1.1287004232406617, 0.8043519592285157, 0.5595008885860443, 0.4100564992427826]\n",
      "\t\tAlpha: 1.7559796627187951\n",
      "\t\tEst Iter: 100\n",
      "\t\tFinal Loss: 0.33259990581477705\n",
      "--------------------------------------------------------------------------------\n",
      "Task segment_semantic:\n",
      "\t\tLoss Samples: [3.09397216796875, 2.61767041683197, 2.423229441642761, 2.24972758769989]\n",
      "\t\tAlpha: 0.12717652152661832\n",
      "\t\tEst Iter: 600\n",
      "\t\tFinal Loss: -65.66746332112103\n",
      "Task normal:\n",
      "\t\tLoss Samples: [0.0853645670413971, 0.07236411690711975, 0.06953933477401733, 0.06694257378578186]\n",
      "\t\tAlpha: 0.05513490863019318\n",
      "\t\tEst Iter: 300\n",
      "\t\tFinal Loss: -0.9615056156308512\n",
      "Task depth_zbuffer:\n",
      "\t\tLoss Samples: [1.2136949682235718, 0.8251208317279816, 0.591980453133583, 0.44738956570625305]\n",
      "\t\tAlpha: 0.9351841124809309\n",
      "\t\tEst Iter: 100\n",
      "\t\tFinal Loss: 0.07687245905262034\n",
      "--------------------------------------------------------------------------------\n",
      "Task segment_semantic:\n",
      "\t\tLoss Samples: [3.268626298904419, 2.8803008365631104, 2.728899693489075, 2.611032485961914]\n",
      "\t\tAlpha: 0.2658152225680651\n",
      "\t\tEst Iter: 1050\n",
      "\t\tFinal Loss: -40.23827014015336\n",
      "Task normal:\n",
      "\t\tLoss Samples: [0.0824106514453888, 0.06799387097358704, 0.06634557723999024, 0.0652143907546997]\n",
      "\t\tAlpha: 0.1735979542821439\n",
      "\t\tEst Iter: 450\n",
      "\t\tFinal Loss: -0.35078226575266735\n",
      "Task depth_zbuffer:\n",
      "\t\tLoss Samples: [1.0009835433959962, 0.7168838047981262, 0.5752307713031769, 0.4498671293258667]\n",
      "\t\tAlpha: 0.17553404301997755\n",
      "\t\tEst Iter: 100\n",
      "\t\tFinal Loss: -48.164555644662606\n",
      "--------------------------------------------------------------------------------\n",
      "Task segment_semantic:\n",
      "\t\tLoss Samples: [3.2833314895629884, 2.843341555595398, 2.712303333282471, 2.616653633117676]\n",
      "\t\tAlpha: 0.2598911599383562\n",
      "\t\tEst Iter: 1300\n",
      "\t\tFinal Loss: -31.47137720663919\n",
      "Task normal:\n",
      "\t\tLoss Samples: [0.09773954272270202, 0.08028552651405335, 0.07809475541114808, 0.07610989809036255]\n",
      "\t\tAlpha: 0.04756217441816322\n",
      "\t\tEst Iter: 550\n",
      "\t\tFinal Loss: -0.7099794952385425\n",
      "Task depth_zbuffer:\n",
      "\t\tLoss Samples: [1.22077214717865, 0.9220438361167907, 0.6856713545322418, 0.5092793798446655]\n",
      "\t\tAlpha: 1.2501844681077487\n",
      "\t\tEst Iter: 100\n",
      "\t\tFinal Loss: 0.23198943257001509\n",
      "--------------------------------------------------------------------------------\n",
      "Task segment_semantic:\n",
      "\t\tLoss Samples: [3.335477018356323, 2.9527987957000734, 2.824136567115784, 2.7064609289169312]\n",
      "\t\tAlpha: 0.08188830951957225\n",
      "\t\tEst Iter: 1050\n",
      "\t\tFinal Loss: -43.757146105127696\n",
      "Task normal:\n",
      "\t\tLoss Samples: [0.09267311096191407, 0.0708516538143158, 0.0696173655986786, 0.06848361134529114]\n",
      "\t\tAlpha: 0.029578055641212207\n",
      "\t\tEst Iter: 550\n",
      "\t\tFinal Loss: -0.3815836911804268\n",
      "Task depth_zbuffer:\n",
      "\t\tLoss Samples: [1.2562793385982514, 0.9313794887065887, 0.6211643528938293, 0.4750726556777954]\n",
      "\t\tAlpha: 16.281433375124337\n",
      "\t\tEst Iter: 100\n",
      "\t\tFinal Loss: 0.47507196386738565\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "Layout 1\n",
      "Fined Layout:\n",
      "[[{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}], [{1, 2}, {0}]]\n",
      "Task segment_semantic:\n",
      "\t\tLoss Samples: [3.2590750217437745, 2.8618109464645385, 2.716565937995911, 2.597833890914917]\n",
      "\t\tAlpha: 0.2003149933103278\n",
      "\t\tEst Iter: 1000\n",
      "\t\tFinal Loss: -42.33237852561036\n",
      "Task normal:\n",
      "\t\tLoss Samples: [0.08066091775894164, 0.0687460708618164, 0.06745717883110046, 0.06664883732795715]\n",
      "\t\tAlpha: 0.20978108486024868\n",
      "\t\tEst Iter: 700\n",
      "\t\tFinal Loss: -0.21761644064344907\n",
      "Task depth_zbuffer:\n",
      "\t\tLoss Samples: [1.273283166885376, 0.9124573838710784, 0.6134604990482331, 0.43232489466667173]\n",
      "\t\tAlpha: 2.6664270656334237\n",
      "\t\tEst Iter: 100\n",
      "\t\tFinal Loss: 0.38337397653234095\n",
      "--------------------------------------------------------------------------------\n",
      "Task segment_semantic:\n",
      "\t\tLoss Samples: [3.295837459564209, 2.8870171737670898, 2.688849477767944, 2.5131722116470336]\n",
      "\t\tAlpha: 0.1663509253204573\n",
      "\t\tEst Iter: 700\n",
      "\t\tFinal Loss: -65.74649998834359\n",
      "Task normal:\n",
      "\t\tLoss Samples: [0.09857009649276734, 0.07236839652061462, 0.07061428427696229, 0.06920946717262268]\n",
      "\t\tAlpha: 0.0821254242993845\n",
      "\t\tEst Iter: 500\n",
      "\t\tFinal Loss: -0.47891121985812873\n",
      "Task depth_zbuffer:\n",
      "\t\tLoss Samples: [1.0660527312755586, 0.7605742633342742, 0.5550095057487487, 0.40679132580757144]\n",
      "\t\tAlpha: 0.8257023760735607\n",
      "\t\tEst Iter: 100\n",
      "\t\tFinal Loss: -12.466150565778554\n",
      "--------------------------------------------------------------------------------\n",
      "Task segment_semantic:\n",
      "\t\tLoss Samples: [3.2351028871536256, 2.7949769496917725, 2.585556888580322, 2.4061461305618286]\n",
      "\t\tAlpha: 0.20824056148843315\n",
      "\t\tEst Iter: 650\n",
      "\t\tFinal Loss: -66.15482281250951\n",
      "Task normal:\n",
      "\t\tLoss Samples: [0.08985409379005432, 0.06977283477783203, 0.06798873305320739, 0.06677194476127625]\n",
      "\t\tAlpha: 0.15808362553156474\n",
      "\t\tEst Iter: 500\n",
      "\t\tFinal Loss: -0.38394641391499196\n",
      "Task depth_zbuffer:\n",
      "\t\tLoss Samples: [1.1126419377326966, 0.7341055047512054, 0.508587052822113, 0.40897689163684847]\n",
      "\t\tAlpha: 1.5777591921354317\n",
      "\t\tEst Iter: 100\n",
      "\t\tFinal Loss: 0.3778015243324575\n",
      "--------------------------------------------------------------------------------\n",
      "Task segment_semantic:\n",
      "\t\tLoss Samples: [3.2023232412338256, 2.642152419090271, 2.4344116115570067, 2.25244327545166]\n",
      "\t\tAlpha: 0.1335333550165163\n",
      "\t\tEst Iter: 600\n",
      "\t\tFinal Loss: -68.708087894495\n",
      "Task normal:\n",
      "\t\tLoss Samples: [0.10088499188423157, 0.07393543720245362, 0.07198372483253479, 0.07086209058761597]\n",
      "\t\tAlpha: 0.21099640833972147\n",
      "\t\tEst Iter: 750\n",
      "\t\tFinal Loss: -0.31412487024405705\n",
      "Task depth_zbuffer:\n",
      "\t\tLoss Samples: [1.1034930515289307, 0.6945119559764862, 0.5276598209142684, 0.4361968958377838]\n",
      "\t\tAlpha: 0.6705336803405797\n",
      "\t\tEst Iter: 100\n",
      "\t\tFinal Loss: -10.360862465972996\n",
      "--------------------------------------------------------------------------------\n",
      "Task segment_semantic:\n",
      "\t\tLoss Samples: False\n",
      "\t\tBad Loss Samples\n",
      "Task normal:\n",
      "\t\tLoss Samples: [0.08678790807723999, 0.06791746377944946, 0.06644517898559571, 0.06564471483230591]\n",
      "\t\tAlpha: 0.2388989494708166\n",
      "\t\tEst Iter: 650\n",
      "\t\tFinal Loss: -0.1975164434094635\n",
      "Task depth_zbuffer:\n",
      "\t\tLoss Samples: [1.2435466599464418, 0.9034334301948548, 0.6890000557899475, 0.49803346931934356]\n",
      "\t\tAlpha: 0.2512589570585022\n",
      "\t\tEst Iter: 100\n",
      "\t\tFinal Loss: -72.60974321693082\n",
      "--------------------------------------------------------------------------------\n",
      "Task segment_semantic:\n",
      "\t\tLoss Samples: [3.3611992692947386, 3.033165678977966, 2.8728511428833006, 2.739985885620117]\n",
      "\t\tAlpha: 0.26230156809691557\n",
      "\t\tEst Iter: 1000\n",
      "\t\tFinal Loss: -46.727516993140135\n",
      "Task normal:\n",
      "\t\tLoss Samples: False\n",
      "\t\tBad Loss Samples\n",
      "Task depth_zbuffer:\n",
      "\t\tLoss Samples: [1.0403746521472932, 0.7518536484241486, 0.5330711954832077, 0.3926793074607849]\n",
      "\t\tAlpha: 1.6033833266003137\n",
      "\t\tEst Iter: 100\n",
      "\t\tFinal Loss: 0.2979844547526282\n",
      "--------------------------------------------------------------------------------\n",
      "Task segment_semantic:\n",
      "\t\tLoss Samples: False\n",
      "\t\tBad Loss Samples\n",
      "Task normal:\n",
      "\t\tLoss Samples: [0.08653884410858154, 0.07073133826255798, 0.06854965448379517, 0.06698867201805114]\n",
      "\t\tAlpha: 0.1690484682343268\n",
      "\t\tEst Iter: 400\n",
      "\t\tFinal Loss: -0.513398005628269\n",
      "Task depth_zbuffer:\n",
      "\t\tLoss Samples: [1.4352073049545289, 0.9071062922477722, 0.6448014104366302, 0.476948014497757]\n",
      "\t\tAlpha: 0.6379383414628088\n",
      "\t\tEst Iter: 100\n",
      "\t\tFinal Loss: -30.071282707067567\n",
      "--------------------------------------------------------------------------------\n",
      "Task segment_semantic:\n",
      "\t\tLoss Samples: [3.3251321840286256, 2.921088309288025, 2.7388654708862306, 2.5744333171844485]\n",
      "\t\tAlpha: 0.12901313993093796\n",
      "\t\tEst Iter: 750\n",
      "\t\tFinal Loss: -61.88160611404148\n",
      "Task normal:\n",
      "\t\tLoss Samples: [0.09764843583106994, 0.07648769974708557, 0.07486557841300964, 0.07392971158027649]\n",
      "\t\tAlpha: 0.2141466040180118\n",
      "\t\tEst Iter: 1100\n",
      "\t\tFinal Loss: -0.24673409022042847\n",
      "Task depth_zbuffer:\n",
      "\t\tLoss Samples: False\n",
      "\t\tBad Loss Samples\n",
      "--------------------------------------------------------------------------------\n",
      "Task segment_semantic:\n",
      "\t\tLoss Samples: False\n",
      "\t\tBad Loss Samples\n",
      "Task normal:\n",
      "\t\tLoss Samples: [0.0997374427318573, 0.07553702235221862, 0.07370884656906128, 0.07208025336265564]\n",
      "\t\tAlpha: 0.04475406583781333\n",
      "\t\tEst Iter: 550\n",
      "\t\tFinal Loss: -0.5725991662317649\n",
      "Task depth_zbuffer:\n",
      "\t\tLoss Samples: [1.1505621075630188, 0.8437560200691223, 0.6148981928825379, 0.4619310903549194]\n",
      "\t\tAlpha: 1.3744713365033572\n",
      "\t\tEst Iter: 100\n",
      "\t\tFinal Loss: 0.31457055337781975\n",
      "--------------------------------------------------------------------------------\n",
      "Task segment_semantic:\n",
      "\t\tLoss Samples: [3.289856724739075, 2.8770498991012574, 2.6980604219436644, 2.5526012992858886]\n",
      "\t\tAlpha: 0.2482274109839001\n",
      "\t\tEst Iter: 850\n",
      "\t\tFinal Loss: -51.510786185727696\n",
      "Task normal:\n",
      "\t\tLoss Samples: [0.09911012291908264, 0.07060914278030396, 0.06909311652183532, 0.06807771682739258]\n",
      "\t\tAlpha: 0.13661599933064042\n",
      "\t\tEst Iter: 650\n",
      "\t\tFinal Loss: -0.31122634254187326\n",
      "Task depth_zbuffer:\n",
      "\t\tLoss Samples: [1.1539123463630676, 0.7653298985958099, 0.5414601683616638, 0.40054034352302553]\n",
      "\t\tAlpha: 0.8393883868483224\n",
      "\t\tEst Iter: 100\n",
      "\t\tFinal Loss: -4.90665961499896\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "Layout 2\n",
      "Fined Layout:\n",
      "[[{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}], [{0}, {2}, {1}]]\n",
      "Task segment_semantic:\n",
      "\t\tLoss Samples: False\n",
      "\t\tBad Loss Samples\n",
      "Task normal:\n",
      "\t\tLoss Samples: [0.10019916772842408, 0.0729935371875763, 0.0714138913154602, 0.07042857408523559]\n",
      "\t\tAlpha: 0.1658310942730996\n",
      "\t\tEst Iter: 800\n",
      "\t\tFinal Loss: -0.28662164368355086\n",
      "Task depth_zbuffer:\n",
      "\t\tLoss Samples: [1.0719051957130432, 0.7913907444477082, 0.5896149218082428, 0.405862175822258]\n",
      "\t\tAlpha: 0.2839918413488029\n",
      "\t\tEst Iter: 100\n",
      "\t\tFinal Loss: -70.06600468812978\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "seed = 10\n",
    "total_iters, short_iters, start, step, batch_num = 20000, 200, 0, 50, 10\n",
    "load_weight = False\n",
    "smooth_weight, target_ratio = 0.0, 0.8\n",
    "\n",
    "\n",
    "layout_est = []\n",
    "layout_idx = -1\n",
    "# For each layout\n",
    "for L in layout_list:\n",
    "    set_seed(seed)\n",
    "    \n",
    "    layout_idx += 1\n",
    "    print('Layout {}'.format(layout_idx))\n",
    "    layout = coarse_to_fined(L, fined_B, mapping)\n",
    "    print('Fined Layout:', flush=True)\n",
    "    print(layout, flush=True)\n",
    "    \n",
    "    mtl_model = MTSeqModel(prototxt, layout=layout, feature_dim=feature_dim, cls_num=cls_num, verbose=False)\n",
    "    if load_weight:\n",
    "        # Step 1: create weight init state_dict\n",
    "        mtl_init = OrderedDict()\n",
    "        for name, module in mtl_model.named_modules():\n",
    "            if isinstance(module, ComputeBlock):\n",
    "                task_set = module.task_set\n",
    "                layer_idx = module.layer_idx\n",
    "                if len(task_set) > 1:\n",
    "                    merge_flag = True\n",
    "                else:\n",
    "                    # Type 1: save the whole block weights from the corresponding ind. model when no merging\n",
    "                    merge_flag = False\n",
    "                    for block in model.backbone.mtl_blocks:\n",
    "                        if task_set == block.task_set and block.layer_idx == layer_idx:\n",
    "                            for ind_name, param in block.named_parameters():\n",
    "                                mtl_init['.'.join([name, ind_name])] = param  \n",
    "                            # for BN running mean and running var\n",
    "                            for ind_name, param in block.named_buffers():\n",
    "                                mtl_init['.'.join([name, ind_name])] = param\n",
    "\n",
    "            # # Type 2: when the current block have merged operators, save mean weights for convs\n",
    "            elif isinstance(module, Conv2dNode) and merge_flag: \n",
    "                task_convs = [] # store conv weights from task's ind. block\n",
    "                for task in task_set:\n",
    "                    # identify task-corresponding block in the well-trained ind. models \n",
    "                    for block in model.backbone.mtl_blocks:\n",
    "                        if task in block.task_set and block.layer_idx == layer_idx:\n",
    "                            task_module = block.compute_nodes[int(name.split('.')[-1])]  \n",
    "                            temp_weight = task_module.basicOp.weight # no channel alignment or no align variable\n",
    "                            if align_choice == 1 and task_module.out_ord is not None: # simple alignment\n",
    "                                temp_weight = temp_weight[task_module.out_ord]\n",
    "                            elif align_choice == 2: # complex alignment\n",
    "                                if task_module.in_ord is not None:\n",
    "                                    temp_weight = temp_weight[:,task_module.in_ord]\n",
    "                                if task_module.out_ord is not None: \n",
    "                                    temp_weight = temp_weight[task_module.out_ord]\n",
    "                            task_convs.append(temp_weight)\n",
    "                weight_anchor = torch.mean(torch.stack(task_convs),dim=0)\n",
    "                mtl_init[name+'.basicOp.weight'] = weight_anchor\n",
    "\n",
    "            # Type 3: save heads' weights\n",
    "            elif 'heads' in name and isinstance(module, ASPPHeadNode): \n",
    "                ind_head = model.heads[name.split('.')[-1]]\n",
    "                for ind_name, param in ind_head.named_parameters():\n",
    "                    mtl_init['.'.join([name, ind_name])] = param\n",
    "                for ind_name, param in ind_head.named_buffers():\n",
    "                    mtl_init['.'.join([name, ind_name])] = param\n",
    "        mtl_model.load_state_dict(mtl_init,strict=False)\n",
    "        print('Finish Weight Loading.', flush=True)\n",
    "        print('-'*80)\n",
    "    \n",
    "    mtl_model = mtl_model.cuda()\n",
    "    mtl_model.train()\n",
    "    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, mtl_model.parameters()), lr=0.01)\n",
    "    \n",
    "    # For each task\n",
    "    alpha_lst = {task: [] for task in tasks}\n",
    "    est_iter_lst ={task: [] for task in tasks}\n",
    "    final_loss_lst = {task: [] for task in tasks}\n",
    "    \n",
    "    # Step 2: Save short train loss list\n",
    "    for idx, data in enumerate(trainDataloaderFix):\n",
    "        if idx >= batch_num:\n",
    "            break\n",
    "        x = data['input'].cuda()\n",
    "        if load_weight:\n",
    "            mtl_model.load_state_dict(mtl_init,strict=False)\n",
    "        else:\n",
    "            mtl_model.reset_parameters()\n",
    "        \n",
    "        loss_lst = {task: [] for task in tasks}\n",
    "        for it in range(short_iters):\n",
    "            optimizer.zero_grad()\n",
    "            output = mtl_model(x)\n",
    "            loss = 0\n",
    "            for task in tasks:\n",
    "                y = data[task].cuda()\n",
    "                if task + '_mask' in data:\n",
    "                    tloss = criterionDict[task](output[task], y, data[task + '_mask'].cuda())\n",
    "                else:\n",
    "                    tloss = criterionDict[task](output[task], y)\n",
    "                loss_lst[task].append(tloss.item())\n",
    "                loss += tloss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        for task in tasks:\n",
    "            print('Task {}:'.format(task))\n",
    "            sm_loss_lst = smooth(loss_lst[task], smooth_weight)\n",
    "\n",
    "            # Step 3: Take smoothed loss samples from window slices\n",
    "            loss_samples = window_loss_samples(sm_loss_lst, start, step=step)\n",
    "            print('\\t\\tLoss Samples: {}'.format(loss_samples))\n",
    "            if loss_samples == False:\n",
    "                print('\\t\\tBad Loss Samples')\n",
    "                continue\n",
    "\n",
    "            # Step 4: Compute convergence rate \n",
    "            alpha = compute_alpha2(loss_samples)\n",
    "            alpha_lst[task].append(alpha)\n",
    "            print('\\t\\tAlpha: {}'.format(alpha))\n",
    "\n",
    "            # Step 5,6: Estimate final loss after 20000 iters and iters to reach target loss\n",
    "            n = (total_iters - start)//step\n",
    "            est_n, final_loss = est_final_loss2(loss_samples, n, alpha, target[task]*target_ratio)\n",
    "            if est_n != -1:\n",
    "                est_iter = start + est_n*step\n",
    "            else:\n",
    "                est_iter = est_n\n",
    "            print('\\t\\tEst Iter: {}'.format(est_iter))\n",
    "            print('\\t\\tFinal Loss: {}'.format(final_loss))\n",
    "            est_iter_lst[task].append(est_iter)\n",
    "            final_loss_lst[task].append(final_loss)\n",
    "        print('-'*80)\n",
    "           \n",
    "    layout_est.append({'alpha':alpha_lst, 'est_iter': est_iter_lst, 'est_loss': final_loss_lst})\n",
    "    print('='*80)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d79b3cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'alpha': {'segment_semantic': [0.08767862140094398,\n",
       "    0.16889808907113696,\n",
       "    0.27173512013653806,\n",
       "    0.13678814705173523,\n",
       "    0.21557560522823227],\n",
       "   'normal': [0.10529386608303633,\n",
       "    0.009307225004601889,\n",
       "    0.10804536014145633,\n",
       "    0.16873231107424447,\n",
       "    0.140873784388299],\n",
       "   'depth_zbuffer': [0.6728202011950046,\n",
       "    1.7250111370604329,\n",
       "    0.3633880413110632,\n",
       "    0.37397310308029286,\n",
       "    1.2183369648647833]},\n",
       "  'est_iter': {'segment_semantic': [1000, 1000, 750, 600, 950],\n",
       "   'normal': [700, 100, 100, 100, 100],\n",
       "   'depth_zbuffer': [100, 100, 100, 100, 100]},\n",
       "  'est_loss': {'segment_semantic': [-42.52371637728947,\n",
       "    -38.834884491691994,\n",
       "    -50.80944338488352,\n",
       "    -66.52761254552476,\n",
       "    -41.640885575688145],\n",
       "   'normal': [-0.3945712930723772,\n",
       "    -0.349581303161049,\n",
       "    -0.3808884450914222,\n",
       "    -0.6318260920549031,\n",
       "    -0.4086360111280515],\n",
       "   'depth_zbuffer': [-24.06044101768165,\n",
       "    0.39352482945122924,\n",
       "    -49.70203580426662,\n",
       "    -46.607082384837746,\n",
       "    0.34407622312657365]}}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d1236140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.6645989847183227, 3.5812311005592345, 3.485876207590103, 3.392624996685982]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "loss_samples = window_loss_samples(loss_lst['segment_semantic'], start, step=step, smooth_weight=0.1)\n",
    "# loss_samples = window_loss_samples(loss_lst['segment_semantic'], start, indices=[10,20,30,50], smooth_weight=0.1)\n",
    "print(loss_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "800df536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, -143.93745662272735)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss_samples=[2.9,2.4,2.1,1.99]\n",
    "alpha = compute_alpha2(loss_samples)\n",
    "est_final_loss2(loss_samples, 1000, alpha, target['segment_semantic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9b64d69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6756569368775177"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093efa19",
   "metadata": {},
   "source": [
    "## helper functions in algs_EstCon.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b431159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(scalars, weight):  # Weight between 0 and 1\n",
    "    last = scalars[0]  # First value in the plot (first timestep)\n",
    "    smoothed = list()\n",
    "    for point in scalars:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  # Calculate smoothed value\n",
    "        smoothed.append(smoothed_val)                        # Save it\n",
    "        last = smoothed_val                                  # Anchor the last smoothed value\n",
    "    return smoothed\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de0d61b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_loss_samples(loss_lst, start, step=None, indices=None):\n",
    "    if (step != None and start + step * 4 > len(loss_lst)) or (indices !=None and len(indices) != 4):\n",
    "        print('Wrong Window Slices for Loss Samples!')\n",
    "        return False\n",
    "    if step != None:\n",
    "        samples = [np.mean(loss_lst[(start+step*i):(start+step*(i+1))]) for i in range(4)]\n",
    "    elif indices !=None:\n",
    "        samples = [np.mean(loss_lst[start:indices[i]]) if i == 0 else np.mean(loss_lst[indices[i-1]:indices[i]]) for i in range(4)]\n",
    "    else:\n",
    "        print('No Slices or Step designed for Loss Samples!')\n",
    "        return False\n",
    "    if judge_loss_samples(samples):\n",
    "        return samples\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def judge_loss_samples(samples):\n",
    "    diff_prev = 1000\n",
    "    for i in range(1, len(samples)):\n",
    "        prev = samples[i-1]\n",
    "        cur = samples[i]\n",
    "        diff = prev - cur\n",
    "        if prev < cur or diff > diff_prev:\n",
    "            return False\n",
    "        else:\n",
    "            diff_prev = diff\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00efd18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_loss_samples(loss_lst, indices, tol=10):\n",
    "    if len(indices) != 4:\n",
    "        print('Wrong Indices for Loss Samples!')\n",
    "        return False\n",
    "    # sample 0\n",
    "    samples = [loss_lst[indices[0]]]\n",
    "    # sample 1,2,3\n",
    "    diff_prev = 1000\n",
    "    for idx in range(1,len(indices)):\n",
    "        prev = samples[idx-1]\n",
    "        cur = loss_lst[indices[idx]]\n",
    "        diff = prev - cur\n",
    "        if cur < prev and diff < diff_prev:\n",
    "            samples.append(cur)\n",
    "            diff_prev = diff\n",
    "        else:\n",
    "            for i in range(indices[idx]-tol,indices[idx]+tol):\n",
    "                prev = samples[idx-1]\n",
    "                cur = loss_lst[i]\n",
    "                diff = prev - cur\n",
    "                if cur < prev and diff < diff_prev:\n",
    "                    samples.append(cur)\n",
    "                    diff_prev = diff\n",
    "                    break\n",
    "                else:\n",
    "                    return False\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1248729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alpha(loss_samples):\n",
    "    return np.log(loss_samples[2]/loss_samples[1])/ np.log(loss_samples[1]/loss_samples[0])\n",
    "\n",
    "def compute_alpha2(loss_samples):\n",
    "    return np.log(np.abs((loss_samples[3]-loss_samples[2])/(loss_samples[2]-loss_samples[1])))/ \\\n",
    "            np.log(np.abs((loss_samples[2]-loss_samples[1])/(loss_samples[1]-loss_samples[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30e0c604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def est_recov_n(loss_samples, target, alpha):\n",
    "    up = np.log(target/loss_samples[0]) * (alpha - 1)\n",
    "    down = np.log(loss_samples[1]/loss_samples[0])\n",
    "    return np.log(up/down + 1) / np.log(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcc30c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def est_final_loss(loss_samples, n, alpha):\n",
    "    x0, x1 = np.log(loss_samples[0]), np.log(loss_samples[1])\n",
    "    for i in range(2, n+1):\n",
    "        x2 = alpha * (x1-x0) + x1\n",
    "        x0 = x1\n",
    "        x1 = x2\n",
    "    return np.exp(x2)\n",
    "\n",
    "def est_final_loss2(loss_samples, n, alpha, target):\n",
    "    x0, x1 = np.log(loss_samples[0]-loss_samples[1]), np.log(loss_samples[1]-loss_samples[2])\n",
    "    temp = loss_samples[2]\n",
    "    est_n = -1\n",
    "    flag = True\n",
    "    \n",
    "    for i in range(2, n+1):\n",
    "        x2 = alpha * (x1-x0) + x1\n",
    "        if np.isinf(x2):\n",
    "            break\n",
    "        else:\n",
    "            temp -= np.exp(x2)\n",
    "        if temp < target and flag:\n",
    "            est_n = i\n",
    "            flag = False\n",
    "        x0 = x1\n",
    "        x1 = x2\n",
    "    return est_n, temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-resource",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-retention",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-franklin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-disability",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.conda-multitask)",
   "language": "python",
   "name": "conda-env-.conda-multitask-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
